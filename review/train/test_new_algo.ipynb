{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training&Evaluation of a developed algorithm\n",
    "\n",
    "This notebook contains source code for several possible architectures to train & evaluate them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from review import config as cfg\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('CUDA version', torch.version.cuda)\n",
    "print('CUDA is available', torch.cuda.is_available())\n",
    "\n",
    "\n",
    "def setup_cuda_device(model):\n",
    "    logging.info('Setup single-device settings...')\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    return model, device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logging.info('Fix seed')\n",
    "seed = cfg.seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading data and preparation of dataset\n",
    "\n",
    "`load_data` loads all the needed datafiles for building train dataset.\\\n",
    "The several next steps are only should be done if no train/test/val datasets are saved."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def load_data(additional_features):\n",
    "    dataset_root = os.path.expanduser(cfg.dataset_path)\n",
    "    logging.info(f'Loading references dataset from {dataset_root}')\n",
    "\n",
    "    logging.info('Loading citations_df')\n",
    "    citations_df = pd.read_csv(os.path.join(dataset_root, \"citations.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(citations_df)))\n",
    "\n",
    "    logging.info('Loading sentences_df')\n",
    "    sentences_df = pd.read_csv(os.path.join(dataset_root, \"sentences.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(sentences_df)))\n",
    "\n",
    "    logging.info('Loading review_files_df')\n",
    "    review_files_df = pd.read_csv(os.path.join(dataset_root, \"review_files.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(review_files_df)))\n",
    "\n",
    "    logging.info('Loading reverse_ref_df')\n",
    "    reverse_ref_df = pd.read_csv(os.path.join(dataset_root, \"reverse_ref.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(reverse_ref_df)))\n",
    "\n",
    "    if not additional_features:\n",
    "        return citations_df, sentences_df, review_files_df, reverse_ref_df, None, None, None\n",
    "\n",
    "    logging.info('Loading abstracts_df')\n",
    "    abstracts_df = pd.read_csv(os.path.join(dataset_root, \"abstracts.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(abstracts_df)))\n",
    "\n",
    "    logging.info('Loading figures_df')\n",
    "    figures_df = pd.read_csv(os.path.join(dataset_root, \"figures.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(figures_df)))\n",
    "\n",
    "    logging.info('Loading tables_df')\n",
    "    tables_df = pd.read_csv(os.path.join(dataset_root, \"tables.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(tables_df)))\n",
    "\n",
    "    return citations_df, sentences_df, review_files_df, reverse_ref_df, abstracts_df, figures_df, tables_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ADDITIONAL_FEATURES = True\n",
    "\n",
    "citations_df, sentences_df, review_files_df, reverse_ref_df, abstracts_df, figures_df, tables_df = \\\n",
    "    load_data(additional_features=ADDITIONAL_FEATURES)\n",
    "\n",
    "logging.info('Done loading references dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "res = Counter(list(sentences_df['pmid'].values))\n",
    "plt.hist(res.values(), bins=range(-1, 400))\n",
    "plt.title('Length of papers')\n",
    "plt.show()\n",
    "del res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "REF_SENTS_DF_PATH = f\"{os.path.expanduser(cfg.base_path)}/ref_sents.csv\"\n",
    "! rm {REF_SENTS_DF_PATH}\n",
    "\n",
    "if os.path.exists(REF_SENTS_DF_PATH):\n",
    "    ref_sents_df = pd.read_csv(REF_SENTS_DF_PATH, sep='\\t')\n",
    "else:\n",
    "    logging.info('Creating reference sentences dataset')\n",
    "    ref_sents_df = pd.merge(citations_df, reverse_ref_df, left_on=['pmid', 'ref_id'], right_on=['pmid', 'ref_id'])\n",
    "    ref_sents_df = pd.merge(ref_sents_df, sentences_df, left_on=['pmid', 'sent_type', 'sent_id'],\n",
    "                            right_on=['pmid', 'type', 'sent_id'])\n",
    "    ref_sents_df = ref_sents_df[ref_sents_df['pmid'].isin(review_files_df['pmid'].values)]\n",
    "    ref_sents_df = ref_sents_df.drop_duplicates()\n",
    "    logging.info(f'Len of unique ref_sents {len(set(ref_sents_df[\"ref_pmid\"]))}')\n",
    "    ref_sents_df = ref_sents_df[['pmid', 'ref_id', 'pub_type', 'ref_pmid', 'sent_type', 'sent_id', 'sentence']]\n",
    "    ref_sents_df.to_csv(REF_SENTS_DF_PATH, sep='\\t', index=False)\n",
    "\n",
    "logging.info('Cleanup memory')\n",
    "del citations_df\n",
    "del review_files_df\n",
    "\n",
    "display(ref_sents_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rouge\n",
    "`get_rouge` function allows to compute similarity between two sentences.\n",
    "`rouge-l` is another possible option."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "ROUGE_METER = Rouge()\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "def get_rouge(sent1, sent2):\n",
    "    if sent1 is None or sent2 is None:\n",
    "        return None\n",
    "    sent_1 = TOKENIZER.tokenize(sent1)\n",
    "    if len(sent_1) == 0:\n",
    "        return None\n",
    "    sent_1 = \" \".join(list(filter(lambda x: x.isalpha() or x in '.!,?', sent_1)))\n",
    "    sent_2 = TOKENIZER.tokenize(sent2)\n",
    "    if len(sent_2) == 0:\n",
    "        return None\n",
    "    sent_2 = \" \".join(list(filter(lambda x: x.isalpha() or x in '.!,?', sent_2)))\n",
    "    if len(sent_1) == 0 or len(sent_2) == 0:\n",
    "        return None\n",
    "    rouges = ROUGE_METER.get_scores(sent_1, sent_2)[0]\n",
    "    rouges = [rouges[f'rouge-{x}'][\"f\"] for x in ('1', '2')]  # , 'l')]\n",
    "    return np.mean(rouges) * 100\n",
    "\n",
    "\n",
    "def mean_rouge(sent, text):\n",
    "    if len(text) == 0:\n",
    "        return None\n",
    "    try:\n",
    "        return sum(get_rouge(sent, ref_sent) for ref_sent in text) / len(text)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception at mean_rouge {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def min_rouge(sent, text):\n",
    "    try:\n",
    "        score = 100000000\n",
    "        for ref_sent in text:\n",
    "            score = min(get_rouge(sent, ref_sent), score)\n",
    "        if score == 100000000:\n",
    "            return None\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception at min_rouge {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def max_rouge(sent, text):\n",
    "    try:\n",
    "        score = -100000\n",
    "        for ref_sent in text:\n",
    "            score = max(get_rouge(sent, ref_sent), score)\n",
    "        if score == -100000:\n",
    "            return None\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception at max_rouge {e}')\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_rouge(\"I am scout. True!\", \"No you are not a scout.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build train / test/ validate datasets\n",
    "\n",
    "To create a dataset with features use `preprocess_paper_with_features`. Otherwise, use `preprocess_paper`.\n",
    "\n",
    "In case datasets are not yet created and `ref_sents_df` is also not yet created, let's create `ref_sents_df`.\\\n",
    "For each paper pmid there is a list of sentences from review papers in which the paper with this `pmid` is cited."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "REPLACE_SYMBOLS = {\n",
    "    '—': '-',\n",
    "    '–': '-',\n",
    "    '―': '-',\n",
    "    '…': '...',\n",
    "    '´´': \"´\",\n",
    "    '´´´': \"´´\",\n",
    "    \"''\": \"'\",\n",
    "    \"'''\": \"'\",\n",
    "    \"``\": \"`\",\n",
    "    \"```\": \"`\",\n",
    "    \":\": \" : \",\n",
    "}\n",
    "\n",
    "\n",
    "def parse_sents(data):\n",
    "    sents = sum([sent_tokenize(text) for text in data], [])\n",
    "    sents = list(filter(lambda x: len(x) > 3, sents))\n",
    "    return sents\n",
    "\n",
    "\n",
    "def sent_standardize(sent):\n",
    "    sent = unidecode(sent)\n",
    "    sent = re.sub(r\"\\[(xref_\\w*_\\w\\d*]*)(, xref_\\w*_\\w\\d*)*\\]\", \" \", sent)  # delete [xref,...]\n",
    "    sent = re.sub(r\"\\( (xref_\\w*_\\w\\d*)(; xref_\\w*_\\w\\d*)* \\)\", \" \", sent)  # delete (xref; ...)\n",
    "    sent = re.sub(r\"\\[xref_\\w*_\\w\\d*\\]\", \" \", sent)  # delete [xref]\n",
    "    sent = re.sub(r\"xref_\\w*_\\w\\d*\", \" \", sent)  # delete [[xref]]\n",
    "    for k, v in REPLACE_SYMBOLS.items():\n",
    "        sent = sent.replace(k, v)\n",
    "    return sent.strip()\n",
    "\n",
    "\n",
    "def standardize(text):\n",
    "    return [x for x in (sent_standardize(sent) for sent in text) if len(x) > 3]\n",
    "\n",
    "\n",
    "def preprocess_paper(paper_id, sentences_df, ref_sents_df, min_paper_sents=50, max_paper_sents=100):\n",
    "    paper = sentences_df[sentences_df['pmid'] == paper_id]['sentence']\n",
    "    paper = standardize(paper)\n",
    "\n",
    "    ref_sents = ref_sents_df[ref_sents_df['ref_pmid'] == paper_id]['sentence']\n",
    "    ref_sents = standardize(ref_sents)\n",
    "\n",
    "    if len(paper) < min_paper_sents:\n",
    "        return None\n",
    "\n",
    "    if len(paper) > max_paper_sents:\n",
    "        paper = list(paper[:min_paper_sents]) + list(paper[-min_paper_sents:])\n",
    "\n",
    "    preprocessed_score = [\n",
    "        sum(get_rouge(sent, ref_sent) for ref_sent in ref_sents) / len(ref_sents) for sent in paper\n",
    "    ]\n",
    "    return paper, preprocessed_score\n",
    "\n",
    "\n",
    "def preprocess_paper_with_features(paper_id, sentences_df, ref_sents_df,\n",
    "                                   abstracts_df, figures_df, reverse_ref_df, tables_df,\n",
    "                                   min_paper_sents=50, max_paper_sents=100):\n",
    "    preprocessed_score = []\n",
    "    features = []\n",
    "\n",
    "    papers = sentences_df[sentences_df['pmid'] == paper_id]['sentence']\n",
    "    papers = standardize(papers)\n",
    "\n",
    "    sent_ids = sentences_df[sentences_df['pmid'] == paper_id]['sent_id']\n",
    "    sent_types = sentences_df[sentences_df['pmid'] == paper_id]['type']\n",
    "\n",
    "    ref_sents = ref_sents_df[ref_sents_df['ref_pmid'] == paper_id]['sentence']\n",
    "    ref_sents = standardize(ref_sents)\n",
    "\n",
    "    fig_captions = figures_df[figures_df['pmid'] == paper_id]['caption']\n",
    "    fig_captions = standardize(fig_captions)\n",
    "\n",
    "    tab_captions = tables_df[tables_df['pmid'] == paper_id]['caption']\n",
    "    tab_captions = standardize(tab_captions)\n",
    "\n",
    "    abstract = abstracts_df[abstracts_df['pmid'] == paper_id]['abstract']\n",
    "    if len(abstract) != 0:\n",
    "        abstract = standardize(abstract)\n",
    "\n",
    "    tmp_df = reverse_ref_df[reverse_ref_df['pmid'] == paper_id]\n",
    "\n",
    "    if len(papers) < min_paper_sents:\n",
    "        return None\n",
    "\n",
    "    if len(papers) > max_paper_sents:\n",
    "        papers = list(papers[:min_paper_sents]) + list(papers[-min_paper_sents:])\n",
    "        sent_ids = list(sent_ids[:min_paper_sents]) + list(sent_ids[-min_paper_sents:])\n",
    "        sent_types = list(sent_types[:min_paper_sents]) + list(sent_types[-min_paper_sents:])\n",
    "\n",
    "    for sent, sent_type, sent_id in zip(papers, sent_types, sent_ids):\n",
    "        score = mean_rouge(sent, ref_sents)\n",
    "        if score is None:\n",
    "            return None\n",
    "\n",
    "        r_abs = get_rouge(sent, abstract[0])\n",
    "        num_refs = len(tmp_df[(tmp_df['sent_type'] == sent_type) & (tmp_df['sent_id'] == sent_id)])\n",
    "        preprocessed_score.append(score)\n",
    "        features.append((sent_id, int(sent_type == \"general\"), r_abs, num_refs,\n",
    "                         mean_rouge(sent, fig_captions), mean_rouge(sent, tab_captions),\n",
    "                         min_rouge(sent, fig_captions), min_rouge(sent, tab_captions),\n",
    "                         max_rouge(sent, fig_captions), max_rouge(sent, tab_captions)))\n",
    "    return papers, preprocessed_score, features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_reference_sentences_dataset(sentences_df, ref_sents_df, additional_features):\n",
    "    res = {}\n",
    "    inter = set(sentences_df['pmid'].values) & set(ref_sents_df['ref_pmid'].values)\n",
    "    for pmid in tqdm(inter):\n",
    "        try:\n",
    "            if additional_features:\n",
    "                temp = preprocess_paper_with_features(\n",
    "                    pmid, sentences_df, ref_sents_df, abstracts_df, figures_df, reverse_ref_df, tables_df\n",
    "                )\n",
    "            else:\n",
    "                temp = preprocess_paper(pmid, sentences_df, ref_sents_df)\n",
    "        except Exception as e:\n",
    "            logging.warning(f'Error during processing {pmid} {e}')\n",
    "            continue\n",
    "        if temp is None:\n",
    "            logging.warning(f'temp is None for {pmid}')\n",
    "            continue\n",
    "        res[pmid] = temp\n",
    "        print(f\"\\r{pmid} {np.mean(res[pmid][1])}\", end=\"\")\n",
    "\n",
    "    logging.info(f'Successfully preprocessed {len(res)} of {len(inter)} papers')\n",
    "\n",
    "    logging.info(f'Creating train dataset')\n",
    "    feature_names = [\n",
    "        'sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "        'mean_r_fig', 'mean_r_tab', 'min_r_fig', 'min_r_tab', 'max_r_fig', 'max_r_tab'\n",
    "    ]\n",
    "    train_dic = dict(\n",
    "        pmid=[], sentence=[], score=[], sent_id=[], sent_type=[], r_abs=[], num_refs=[],\n",
    "        mean_r_fig=[], mean_r_tab=[], min_r_fig=[], min_r_tab=[], max_r_fig=[], max_r_tab=[]\n",
    "    )\n",
    "\n",
    "    for pmid, stat in tqdm(res.items()):\n",
    "        if len(stat) == 2:\n",
    "            for sent, score in zip(*stat):\n",
    "                train_dic['pmid'].append(pmid)\n",
    "                train_dic['sentence'].append(sent)\n",
    "                train_dic['score'].append(score)\n",
    "        else:\n",
    "            for sent, score, features in zip(*stat):\n",
    "                train_dic['pmid'].append(pmid)\n",
    "                train_dic['sentence'].append(sent)\n",
    "                train_dic['score'].append(score)\n",
    "                for name, val in zip(feature_names, features):\n",
    "                    train_dic[name].append(val)\n",
    "\n",
    "    train_df = pd.DataFrame({k: v for k, v in train_dic.items() if v})\n",
    "    logging.info(f'Full train dataset {len(train_df)}')\n",
    "    return train_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = f'{os.path.expanduser(cfg.base_path)}/dataset.csv'\n",
    "! rm {TRAIN_DATASET_PATH}\n",
    "\n",
    "if os.path.exists(TRAIN_DATASET_PATH):\n",
    "    train_df = pd.read_csv(TRAIN_DATASET_PATH)\n",
    "else:\n",
    "    train_df = process_reference_sentences_dataset(\n",
    "        sentences_df, ref_sents_df, additional_features=ADDITIONAL_FEATURES\n",
    "    )\n",
    "    train_df.to_csv(TRAIN_DATASET_PATH, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(train_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "res = Counter(list(train_df['score'].values))\n",
    "\n",
    "plt.hist(res.values(), bins=range(2, 20))\n",
    "plt.title('Train dataset scores')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Splitting data into train/test/val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(list(set(train_df['pmid'].values)), test_size=0.2)\n",
    "test_ids, val_ids = train_test_split(test_ids, test_size=0.4)\n",
    "\n",
    "train = train_df[train_df['pmid'].isin(train_ids)]\n",
    "logging.info(f'Train {len(train)}')\n",
    "display(train.head(1))\n",
    "\n",
    "test = train_df[train_df['pmid'].isin(test_ids)]\n",
    "logging.info(f'Test {len(test)}')\n",
    "display(test.head(1))\n",
    "\n",
    "val = train_df[train_df['pmid'].isin(val_ids)]\n",
    "logging.info(f'Validate {len(val)}')\n",
    "display(val.head(1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inspect pretrained models: BERT and Roberta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel, RobertaModel\n",
    "\n",
    "backbone = BertModel.from_pretrained(\n",
    "    \"bert-base-uncased\", output_hidden_states=False\n",
    ")\n",
    "print('BERT pretrained')\n",
    "print(f'Parameters {sum(p.numel() for p in backbone.parameters() if p.requires_grad)}')\n",
    "print(', '.join(n for n, p in backbone.named_parameters()))\n",
    "# print(backbone)\n",
    "\n",
    "# backbone = RobertaModel.from_pretrained(\n",
    "#     'roberta-base', output_hidden_states=False\n",
    "# )\n",
    "# print('ROBERTA pretrained')\n",
    "# print(f'Parameters {sum(p.numel() for p in backbone.parameters() if p.requires_grad)}')\n",
    "# print(', '.join(n for n, p in backbone.named_parameters()))\n",
    "# print(backbone)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main model classes\n",
    "\n",
    "The model in main pubtrends application is loaded using `load_model` function from `review.model` module.\n",
    "\n",
    "It has several options to set up:\n",
    "* with or without features (right now without features works better),\n",
    "* `BERT` or `roberta` as basis (no big difference),\n",
    "\n",
    "You can also choose `frozen_strategy`:\n",
    "* `froze_all` in case you don't want to improve bert layers but only the summarization layer,\n",
    "* `unfroze_last4` -- modifies bert weights and still training not very slow,\n",
    "* `unfroze_all` -- the training is slow, the results may better though"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from transformers import BertModel, RobertaModel\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "\n",
    "import review.config as cfg\n",
    "\n",
    "SpecToken = namedtuple('SpecToken', ['tkn', 'idx'])\n",
    "ConvertToken2Id = lambda tokenizer, tkn: tokenizer.convert_tokens_to_ids([tkn])[0]\n",
    "\n",
    "\n",
    "class Summarizer(nn.Module):\n",
    "    enc_output: torch.Tensor\n",
    "    dec_ids_mask: torch.Tensor\n",
    "    encdec_ids_mask: torch.Tensor\n",
    "\n",
    "    def __init__(self, model_type, article_len, additional_features, num_features=10):\n",
    "        super(Summarizer, self).__init__()\n",
    "\n",
    "        self.article_len = article_len\n",
    "        print(f'Initialize backbone and tokenizer for {model_type}')\n",
    "        if model_type == 'bert':\n",
    "            self.backbone = self.initialize_bert()\n",
    "            self.tokenizer = self.initialize_bert_tokenizer()\n",
    "        elif model_type == 'roberta':\n",
    "            self.backbone = self.initialize_roberta()\n",
    "            self.tokenizer = self.initialize_roberta_tokenizer()\n",
    "        else:\n",
    "            raise Exception(f\"Wrong model_type argument: {model_type}\")\n",
    "        self.backbone.resize_token_embeddings(200 + self.tokenizer.vocab_size)\n",
    "\n",
    "        if additional_features:\n",
    "            print('Adding additional features double fully connected nn')\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Linear(num_features, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 50)\n",
    "            )\n",
    "        else:\n",
    "            self.features = None\n",
    "\n",
    "        print('Initialize backbone emb pulling')\n",
    "        def backbone_forward(input_ids, attention_mask, token_type_ids, position_ids):\n",
    "            return self.backbone(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "            )\n",
    "        self.encoder = lambda *args: backbone_forward(*args)[0]\n",
    "\n",
    "        print('Initialize decoder')\n",
    "        if additional_features:\n",
    "            self.decoder = Classifier(cfg.d_hidden + 50)\n",
    "        else:\n",
    "            self.decoder = Classifier(cfg.d_hidden)\n",
    "\n",
    "    def expand_positional_embs_if_need(self):\n",
    "        print('Expand positional embeddings if need')\n",
    "        print(self.backbone.config.max_position_embeddings, self.article_len)\n",
    "        if self.article_len > self.backbone.config.max_position_embeddings:\n",
    "            old_maxlen = self.backbone.config.max_position_embeddings\n",
    "            old_w = self.backbone.embeddings.position_embeddings.weight\n",
    "            logging.info(f'Backbone pos embeddings expanded from {old_maxlen} upto {self.article_len}')\n",
    "            self.backbone.embeddings.position_embeddings = nn.Embedding(\n",
    "                self.article_len, self.backbone.config.hidden_size\n",
    "            )\n",
    "            self.backbone.embeddings.position_embeddings.weight[:old_maxlen].data.copy_(old_w)\n",
    "            self.backbone.config.max_position_embeddings = self.article_len\n",
    "        print('New positional embeddings', self.backbone.config.max_position_embeddings)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_bert():\n",
    "        return BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", output_hidden_states=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_bert_tokenizer():\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        BOS = \"[CLS]\"\n",
    "        EOS = \"[SEP]\"\n",
    "        PAD = \"[PAD]\"\n",
    "        Summarizer._init_tokenizer(tokenizer, BOS, EOS, PAD)\n",
    "        return tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_roberta():\n",
    "        backbone = RobertaModel.from_pretrained(\n",
    "            'roberta-base', output_hidden_states=False\n",
    "        )\n",
    "        print('initialize token type emb, by default roberta doesnt have it')\n",
    "        backbone.config.type_vocab_size = 2\n",
    "        backbone.embeddings.token_type_embeddings = nn.Embedding(2, backbone.config.hidden_size)\n",
    "        backbone.embeddings.token_type_embeddings.weight.data.normal_(\n",
    "            mean=0.0, std=backbone.config.initializer_range\n",
    "        )\n",
    "        return backbone\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_roberta_tokenizer():\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "        BOS = \"<s>\"\n",
    "        EOS = \"</s>\"\n",
    "        PAD = \"<pad>\"\n",
    "        Summarizer._init_tokenizer(tokenizer, BOS, EOS, PAD)\n",
    "        return tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_tokenizer(tokenizer, BOS, EOS, PAD):\n",
    "        print('Initializing tokenizer with special tokens')\n",
    "        PAD = SpecToken(PAD, ConvertToken2Id(tokenizer, PAD))\n",
    "        artBOS = SpecToken(BOS, ConvertToken2Id(tokenizer, BOS))\n",
    "        artEOS = SpecToken(EOS, ConvertToken2Id(tokenizer, EOS))\n",
    "        print('Add special tokens to tokenizer')\n",
    "\n",
    "        tokenizer.add_special_tokens(dict(additional_special_tokens=[\"<sum>\", \"</sent>\", \"</sum>\"]))\n",
    "\n",
    "        sumBOS = SpecToken(\"<sum>\", ConvertToken2Id(tokenizer, \"<sum>\"))\n",
    "        sumEOS = SpecToken(\"</sent>\", ConvertToken2Id(tokenizer, \"</sent>\"))\n",
    "        sumEOA = SpecToken(\"</sum>\", ConvertToken2Id(tokenizer, \"</sum>\"))\n",
    "\n",
    "        print('Configure tokenizer')\n",
    "        tokenizer.PAD = PAD\n",
    "        tokenizer.artBOS = artBOS\n",
    "        tokenizer.artEOS = artEOS\n",
    "        tokenizer.sumBOS = sumBOS\n",
    "        tokenizer.sumEOS = sumEOS\n",
    "        tokenizer.sumEOA = sumEOA\n",
    "        print('Done initializing tokenizer with special tokens')\n",
    "\n",
    "    def save(self, save_filename):\n",
    "        \"\"\" Save model in filename\n",
    "\n",
    "        :param save_filename: str\n",
    "        \"\"\"\n",
    "        state = dict(\n",
    "            encoder_dict=self.backbone.state_dict(),\n",
    "            decoder_dict=self.decoder.state_dict()\n",
    "        )\n",
    "        if self.features:\n",
    "            state['features_dict'] = self.features.state_dict()\n",
    "        models_folder = os.path.expanduser(cfg.weights_path)\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.makedirs(models_folder)\n",
    "        torch.save(state, f\"{models_folder}/{save_filename}.pth\")\n",
    "\n",
    "    def load(self, load_filename):\n",
    "        path = f\"{os.path.expanduser(cfg.weights_path)}/{load_filename}.pth\"\n",
    "        state = torch.load(path, map_location=lambda storage, location: storage)\n",
    "        self.backbone.load_state_dict(state['encoder_dict'])\n",
    "        self.decoder.load_state_dict(state['decoder_dict'])\n",
    "        if self.features:\n",
    "            self.features.load_state_dict(state['features_dict'])\n",
    "\n",
    "    def froze_backbone(self, froze_strategy):\n",
    "        assert froze_strategy in ['froze_all', 'unfroze_last4',\n",
    "                                  'unfroze_all'], f\"incorrect froze_strategy argument: {froze_strategy}\"\n",
    "\n",
    "        if froze_strategy == 'froze_all':\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        elif froze_strategy == 'unfroze_last4':\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                param.requires_grad_(\n",
    "                    'encoder.layer.11' in name or\n",
    "                    'encoder.layer.10' in name or\n",
    "                    'encoder.layer.9' in name or\n",
    "                    'encoder.layer.8' in name\n",
    "                )\n",
    "\n",
    "        elif froze_strategy == 'unfroze_all':\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad_(True)\n",
    "\n",
    "    def unfroze_head(self):\n",
    "        for name, param in self.decoder.named_parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, input_features=None):\n",
    "        \"\"\" Train for 1st stage of model\n",
    "        :param input_ids: torch.Size([batch_size, article_len])\n",
    "        :param attention_mask: torch.Size([batch_size, article_len])\n",
    "        :param token_type_ids: torch.Size([batch_size, article_len])\n",
    "        :return:\n",
    "            logprobs | torch.Size([batch_size, summary_len, vocab_size])\n",
    "        \"\"\"\n",
    "\n",
    "        cls_mask = (input_ids == self.tokenizer.artBOS.idx)\n",
    "\n",
    "        # position ids | torch.Size([batch_size, article_len])\n",
    "        pos_ids = torch.arange(\n",
    "            0,\n",
    "            self.article_len,\n",
    "            dtype=torch.long,\n",
    "            device=input_ids.device\n",
    "        ).unsqueeze(0).repeat(len(input_ids), 1)\n",
    "        # extract bert embeddings | torch.Size([batch_size, article_len, d_bert])\n",
    "        enc_output = self.encoder(input_ids, attention_mask, token_type_ids, pos_ids)\n",
    "\n",
    "        if self.features:\n",
    "            temp_features = self.features(input_features)\n",
    "            draft_logprobs = self.decoder(torch.cat([enc_output[cls_mask], temp_features], dim=-1))\n",
    "        else:\n",
    "            draft_logprobs = self.decoder(enc_output[cls_mask])\n",
    "\n",
    "        return draft_logprobs\n",
    "\n",
    "    def evaluate(self, input_ids, attention_mask, token_type_ids, input_features=None):\n",
    "        \"\"\" Eval for 1st stage of model\n",
    "        :param input_ids: torch.Size([batch_size, article_len])\n",
    "        :param attention_mask: torch.Size([batch_size, article_len])\n",
    "        :param token_type_ids: torch.Size([batch_size, article_len])\n",
    "        :return:\n",
    "            draft_ids | torch.Size([batch_size, summary_len])\n",
    "        \"\"\"\n",
    "\n",
    "        cls_mask = (input_ids == self.tokenizer.artBOS.idx)\n",
    "\n",
    "        # position ids | torch.Size([batch_size, article_len])\n",
    "        pos_ids = torch.arange(\n",
    "            0,\n",
    "            self.article_len,\n",
    "            dtype=torch.long,\n",
    "            device=input_ids.device\n",
    "        ).unsqueeze(0).repeat(len(input_ids), 1)\n",
    "        # extract bert embeddings | torch.Size([batch_size, article_len, d_bert])\n",
    "        enc_output = self.encoder(input_ids, attention_mask, token_type_ids, pos_ids)\n",
    "\n",
    "        ans = []\n",
    "        for eo, cm in zip(enc_output, cls_mask):\n",
    "            if self.features:\n",
    "                scores = self.decoder.evaluate(torch.cat([eo[cm], self.features(input_features)], dim=-1))\n",
    "            else:\n",
    "                scores = self.decoder.evaluate(eo[cm])\n",
    "            ans.append(scores)\n",
    "        return ans\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x).squeeze(-1)\n",
    "        scores = self.sigmoid(x)\n",
    "        return scores\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        x = self.linear1(x).squeeze(-1)\n",
    "        scores = self.sigmoid(x)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def create_model(model_type, froze_strategy, article_len, additional_features):\n",
    "    model = Summarizer(model_type, article_len, additional_features)\n",
    "    model.expand_positional_embs_if_need()\n",
    "    # Load intermediate model\n",
    "    #     model.load('temp')\n",
    "    model.froze_backbone(froze_strategy)\n",
    "    model.unfroze_head()\n",
    "    print(f'Parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing text for BERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_paper_bert(text, max_len, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocess text for Bert / Robert model.\n",
    "    NOTE: not all the text can be processed because of max_len.\n",
    "    :param text: list(list(str))\n",
    "    :param max_len: maximum length of preprocessing\n",
    "    :param tokenizer: Bert of Robert tokenize\n",
    "    :return:\n",
    "        ids | tokenized ids of length max_len, 0 if padding\n",
    "        attention_mask | list(str) 1 if real token, not padding\n",
    "        token_type_ids | 0-1 for different sentences\n",
    "        n_sents | number of actual sentences encoded\n",
    "    \"\"\"\n",
    "    sents = [\n",
    "        [tokenizer.artBOS.tkn] + tokenizer.tokenize(sent) + [tokenizer.artEOS.tkn] for sent in text\n",
    "    ]\n",
    "    logging.debug(f'sents {sents}')\n",
    "    ids, token_type_ids, segment_signature = [], [], 0\n",
    "    n_sents = 0\n",
    "    for i, s in enumerate(sents):\n",
    "        logging.debug(f'sentence {i} {s}')\n",
    "        logging.debug(f'ids {len(ids)}')\n",
    "        logging.debug(f'segments {len(token_type_ids)}')\n",
    "        logging.debug(f'segment_signature {segment_signature}')\n",
    "        if len(ids) + len(s) <= max_len:\n",
    "            n_sents += 1\n",
    "            ids.extend(tokenizer.convert_tokens_to_ids(s))\n",
    "            token_type_ids.extend([segment_signature] * len(s))\n",
    "            segment_signature = (segment_signature + 1) % 2\n",
    "        else:\n",
    "            logging.debug(f'break, len(s)={len(s)}')\n",
    "            break\n",
    "    attention_mask = [1] * len(ids)\n",
    "\n",
    "    logging.debug('Padding data')\n",
    "    pad_len = max(0, max_len - len(ids))\n",
    "    ids += [tokenizer.PAD.idx] * pad_len\n",
    "    attention_mask += [0] * pad_len\n",
    "    token_type_ids += [segment_signature] * pad_len\n",
    "    assert len(ids) == len(attention_mask)\n",
    "    assert len(ids) == len(token_type_ids)\n",
    "    return ids, attention_mask, token_type_ids, n_sents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Inspect preprocessing text for Bert')\n",
    "\n",
    "tokenizer = Summarizer.initialize_bert_tokenizer()\n",
    "text = list(train_df.head(5)['sentence'])\n",
    "input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(text, 512, tokenizer)\n",
    "\n",
    "print(f'input_ids {input_ids}')\n",
    "print(f'attention_mask {attention_mask}')\n",
    "print(f'token_type_ids {token_type_ids}')\n",
    "print(f'n_sents {n_sents}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and evaluate functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "\n",
    "\n",
    "def get_enc_lr(optimizer):\n",
    "    return optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "def get_dec_lr(optimizer):\n",
    "    return optimizer.param_groups[1]['lr']\n",
    "\n",
    "\n",
    "def backward_step(loss: torch.Tensor, optimizer: Optimizer, model: nn.Module, clip: float):\n",
    "    loss.backward()\n",
    "    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def train_fun(\n",
    "        model,\n",
    "        dataloader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        criter,\n",
    "        device,\n",
    "        writer,\n",
    "        additional_features\n",
    "):\n",
    "    # draft, refine\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
    "    for idx_batch, batch in pbar:\n",
    "        batch_on_device = [(x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch]\n",
    "        if additional_features:\n",
    "            input_ids, attention_mask, token_type_ids, target_scores, input_features = batch_on_device\n",
    "            input_features = torch.cat(input_features).to(device)\n",
    "        else:\n",
    "            input_ids, attention_mask, token_type_ids, target_scores = batch_on_device\n",
    "            input_features = None\n",
    "\n",
    "        # forward pass\n",
    "        logging.debug(f'batch {idx_batch}')\n",
    "        logging.debug('forward')\n",
    "        logging.debug(f'input_ids {input_ids.shape}')\n",
    "        logging.debug(f'attention_mask {attention_mask.shape}')\n",
    "        logging.debug(f'token_type_ids {token_type_ids.shape}')\n",
    "\n",
    "        if additional_features:\n",
    "            logging.debug(f'input_features {input_features.shape}')\n",
    "            draft_probs = model(\n",
    "                input_ids, attention_mask, token_type_ids, input_features,\n",
    "            )\n",
    "        else:\n",
    "            draft_probs = model(\n",
    "                input_ids, attention_mask, token_type_ids,\n",
    "            )\n",
    "\n",
    "        logging.debug(f'draft_probs {draft_probs.shape}')\n",
    "        logging.debug(f'target_scores {len(target_scores)}')\n",
    "\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "        try:\n",
    "            # loss\n",
    "            loss = criter(draft_probs, target_scores, )\n",
    "            logging.debug(f'loss {loss}')\n",
    "        except Exception:\n",
    "            print(idx_batch, draft_probs.shape, target_scores.shape, token_type_ids)\n",
    "            return\n",
    "\n",
    "        # backward\n",
    "        logging.debug('backward')\n",
    "        grad_norm = backward_step(loss, optimizer, model, optimizer.clip_value)\n",
    "        grad_norm = 0 if (math.isinf(grad_norm) or math.isnan(grad_norm)) else grad_norm\n",
    "\n",
    "        # record a loss value\n",
    "        # loss_val += loss.item() * len(input_ids)\n",
    "        logging.debug(f'loss {loss.item():.5f}')\n",
    "        pbar.set_description(f\"loss:{loss.item():.5f}\")\n",
    "        writer.add_scalar(f\"Train/loss\", loss.item(), writer.train_step)\n",
    "        writer.add_scalar(\"Train/grad_norm\", grad_norm, writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_enc\", get_enc_lr(optimizer), writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_dec\", get_dec_lr(optimizer), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "        # make a gradient step\n",
    "        if (idx_batch + 1) % optimizer.accumulation_interval == 0 or (idx_batch + 1) == len(dataloader):\n",
    "            logging.debug('optimizer step')\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        logging.debug('scheduler step')\n",
    "        scheduler.step()\n",
    "\n",
    "    # save model, just in case\n",
    "    model.save('temp')\n",
    "    logging.debug('save model to temp')\n",
    "\n",
    "    return model, optimizer, scheduler, writer\n",
    "\n",
    "\n",
    "def evaluate_fun(\n",
    "        model,\n",
    "        dataloader,\n",
    "        criter,\n",
    "        device,\n",
    "        writer,\n",
    "        additional_features\n",
    "):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    loss_val = 0\n",
    "    mean_sents = 0\n",
    "    szs = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), leave=False)\n",
    "    for batch in pbar:\n",
    "        batch_on_device = [(x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch]\n",
    "        if additional_features:\n",
    "            input_ids, attention_mask, token_type_ids, target_scores, input_features = batch_on_device\n",
    "            input_features = torch.cat(input_features).to(device)\n",
    "        else:\n",
    "            input_ids, attention_mask, token_type_ids, target_scores = batch_on_device\n",
    "            input_features = None\n",
    "        sizes = [dc.shape[0] for dc in target_scores]\n",
    "        mean_sents += sum(sizes)\n",
    "        szs += len(sizes)\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "\n",
    "        # forward pass\n",
    "        logging.debug('forward')\n",
    "        logging.debug(f'input_ids {input_ids.shape}')\n",
    "        logging.debug(f'attention_mask {attention_mask.shape}')\n",
    "        logging.debug(f'token_type_ids {token_type_ids.shape}')\n",
    "\n",
    "        if additional_features:\n",
    "            logging.debug(f'input_features {input_features.shape}')\n",
    "            draft_probs = model(\n",
    "                input_ids, attention_mask, token_type_ids, input_features,\n",
    "            )\n",
    "        else:\n",
    "            draft_probs = model(\n",
    "                input_ids, attention_mask, token_type_ids,\n",
    "            )\n",
    "\n",
    "        logging.debug(f'draft_probs {draft_probs.shape}')\n",
    "        logging.debug(f'target_scores {len(target_scores)}')\n",
    "\n",
    "        # loss\n",
    "        loss = criter(draft_probs, target_scores, )\n",
    "\n",
    "        # record a loss value\n",
    "        logging.debug(f\"loss:{loss.item():.5f}\")\n",
    "        pbar.set_description(f\"loss:{loss.item():.5f}\")\n",
    "        loss_val += loss.item()\n",
    "        writer.add_scalar(f\"Eval/loss\", loss.item(), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "    print(\"Val loss:\", loss_val / len(dataloader))\n",
    "    print(\"Mean sent len:\", mean_sents / szs)\n",
    "    # save model, just in case\n",
    "    model.save('validated_weights')\n",
    "    logging.debug('save model to validated_weights')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset and dataloader classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \"\"\" Custom Train Dataset for data with additional features\n",
    "        First preprocess all the data and then give out the batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len, additional_features):\n",
    "        self.df = dataframe\n",
    "        self.data = []\n",
    "        self.pmids = list(set(dataframe['pmid'].values))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "        self.additional_features = additional_features\n",
    "\n",
    "        for pmid in tqdm(self.pmids):\n",
    "            ex = self.df[self.df['pmid'] == pmid]\n",
    "            papers = ex['sentence'].values\n",
    "            features = np.nan_to_num(\n",
    "                ex[['sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "                    'mean_r_fig', 'mean_r_tab',\n",
    "                    'min_r_fig', 'min_r_tab',\n",
    "                    'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "            ) if additional_features else None\n",
    "            total_sents = 0\n",
    "            while total_sents < len(papers):\n",
    "                magic = max(0, total_sents - 5)\n",
    "\n",
    "                input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(\n",
    "                    papers[magic:], self.article_len, self.tokenizer\n",
    "                )\n",
    "                if n_sents <= 5:\n",
    "                    total_sents += 1\n",
    "                    continue\n",
    "                target_scores = ex['score'].values[magic:magic + n_sents] / 100\n",
    "                self.data.append(\n",
    "                    (input_ids, attention_mask, token_type_ids, target_scores, features[magic:magic + n_sents])\n",
    "                    if additional_features else\n",
    "                    (input_ids, attention_mask, token_type_ids, target_scores)\n",
    "                )\n",
    "                total_sents = magic + n_sents\n",
    "\n",
    "        self.n_examples = len(self.data)\n",
    "        logging.debug(f'Train dataset data {self.n_examples}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.additional_features:\n",
    "            article_ids, attention_mask, token_type_ids, target_scores, features = self.data[idx]\n",
    "            logging.debug(f'Train dataset batch {idx}\\n'\n",
    "                          f'input_ids {article_ids}\\n'\n",
    "                          f'attention_mask {attention_mask}\\n'\n",
    "                          f'token_type_ids {token_type_ids}\\n'\n",
    "                          f'target_scores {target_scores}\\n'\n",
    "                          f'features {features}')\n",
    "        else:\n",
    "            article_ids, attention_mask, token_type_ids, target_scores = self.data[idx]\n",
    "            logging.debug(f'Train dataset batch {idx}\\n'\n",
    "                          f'input_ids {article_ids}\\n'\n",
    "                          f'attention_mask {attention_mask}\\n'\n",
    "                          f'token_type_ids {token_type_ids}\\n'\n",
    "                          f'target_scores {target_scores}')\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    \"\"\" Custom Valid/Test Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len, additional_features):\n",
    "        self.df = dataframe\n",
    "        self.n_examples = len(set(dataframe['pmid'].values))\n",
    "        logging.debug(f'Eval dataset examples {self.n_examples}')\n",
    "        self.pmids = list(set(dataframe['pmid'].values))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "        self.additional_features = additional_features\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pmid = self.pmids[idx]\n",
    "        ex = self.df[self.df['pmid'] == pmid]\n",
    "        paper = ex['sentence'].values\n",
    "        features = np.nan_to_num(\n",
    "            ex[['sent_id', 'sent_type', 'r_abs',\n",
    "                'num_refs', 'mean_r_fig', 'mean_r_tab',\n",
    "                'min_r_fig', 'min_r_tab',\n",
    "                'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "        ) if self.additional_features else None\n",
    "\n",
    "        input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(\n",
    "            paper, self.article_len, self.tokenizer\n",
    "        )\n",
    "\n",
    "        # form target\n",
    "        target_scores = ex['score'].values[:n_sents] / 100\n",
    "        if self.additional_features:\n",
    "            logging.debug(f'Eval dataset batch {idx}\\n'\n",
    "                          f'input_ids {input_ids}\\n'\n",
    "                          f'attention_mask {attention_mask}\\n'\n",
    "                          f'token_type_ids {token_type_ids}\\n'\n",
    "                          f'target_scores {target_scores}\\n'\n",
    "                          f'features {features[:n_sents]}')\n",
    "            return input_ids, attention_mask, token_type_ids, target_scores, features[:n_sents]\n",
    "        else:\n",
    "            logging.debug(f'Eval dataset batch {idx}\\n'\n",
    "                          f'input_ids {input_ids}\\n'\n",
    "                          f'attention_mask {attention_mask}\\n'\n",
    "                          f'token_type_ids {token_type_ids}\\n'\n",
    "                          f'target_scores {target_scores}')\n",
    "            return input_ids, attention_mask, token_type_ids, target_scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "\n",
    "def create_collate_fn(additional_features):\n",
    "    \"\"\" Function to pull batch for train\n",
    "    :param batch_data: list of `TrainDataset` or `EvalDataset` Examples\n",
    "    :return:\n",
    "        one batch of data\n",
    "    \"\"\"\n",
    "    def _collate_fn(batch_data):\n",
    "        data = list(zip(*batch_data))\n",
    "        result = [\n",
    "            torch.tensor(data[0], dtype=torch.long),\n",
    "            torch.tensor(data[1], dtype=torch.long),\n",
    "            torch.tensor(data[2], dtype=torch.long),\n",
    "            [torch.tensor(e, dtype=torch.float) for e in data[3]]\n",
    "        ]\n",
    "        if additional_features:\n",
    "            result.append([torch.tensor(e, dtype=torch.float) for e in data[4]])\n",
    "        return result\n",
    "    return _collate_fn\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def get_dataloaders(train, val, batch_size, article_len, tokenizer, additional_features):\n",
    "    logging.info('Creating train dataset...')\n",
    "    train_ds = TrainDataset(train, tokenizer, article_len, additional_features)\n",
    "\n",
    "    logging.info('Applying loader functions to train...')\n",
    "    train_dl = DataLoader(\n",
    "        dataset=train_ds, batch_size=batch_size, shuffle=False,\n",
    "        pin_memory=True, collate_fn=create_collate_fn(additional_features), num_workers=cfg.num_workers\n",
    "    )\n",
    "\n",
    "    logging.info('Creating val dataset...')\n",
    "    val_ds = EvalDataset(val, tokenizer, article_len, additional_features)\n",
    "\n",
    "    logging.info('Applying loader functions to val...')\n",
    "    val_dl = DataLoader(\n",
    "        dataset=val_ds, batch_size=batch_size, shuffle=False,\n",
    "        pin_memory=True, collate_fn=create_collate_fn(additional_features), num_workers=cfg.num_workers\n",
    "    )\n",
    "\n",
    "    return train_dl, val_dl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom scheduler used in training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler, ExponentialLR\n",
    "\n",
    "\n",
    "class CustomScheduler(_LRScheduler):\n",
    "    timestep: int = 0\n",
    "\n",
    "    def __init__(self, optimizer, gamma, warmup=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.after_warmup = ExponentialLR(optimizer, gamma=gamma)\n",
    "        self.initial_lrs = [p_group['lr'] for p_group in self.optimizer.param_groups]\n",
    "        self.warmup = 0 if warmup is None else warmup\n",
    "        super(CustomScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.timestep * group_init_lr / self.warmup for group_init_lr in\n",
    "                self.initial_lrs] if self.timestep < self.warmup else self.after_warmup.get_lr()\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if self.timestep < self.warmup:\n",
    "            self.timestep += 1\n",
    "            super(CustomScheduler, self).step(epoch)\n",
    "        else:\n",
    "            self.after_warmup.step(epoch)\n",
    "\n",
    "\n",
    "class NoamScheduler(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup):\n",
    "        assert warmup > 0\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lrs = [p_group['lr'] for p_group in self.optimizer.param_groups]\n",
    "        self.warmup = warmup\n",
    "        self.timestep = 0\n",
    "        super(NoamScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        noam_lr = self.get_noam_lr()\n",
    "        return [group_init_lr * noam_lr for group_init_lr in self.initial_lrs]\n",
    "\n",
    "    def get_noam_lr(self):\n",
    "        return min(self.timestep ** -0.5, self.timestep * self.warmup ** -1.5)\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        self.timestep += 1\n",
    "        super(NoamScheduler, self).step(epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create learning tools"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import MSELoss\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENCODER_LEARNING_RATE = 0.00001\n",
    "DECODER_LEARNING_RATE = 0.001\n",
    "WARMUP = 5\n",
    "WEIGHT_DECAY = 0.005\n",
    "CLIP_VALUE = 1.0\n",
    "ACCUMULATION_INTERVAL = 1\n",
    "\n",
    "def prepare_learning_tools(\n",
    "        model,\n",
    "        enc_lr = ENCODER_LEARNING_RATE,\n",
    "        dec_lr = DECODER_LEARNING_RATE,\n",
    "        warmup = WARMUP,\n",
    "        weight_decay = WEIGHT_DECAY,\n",
    "        clip_value = CLIP_VALUE,\n",
    "        accumulation_interval = ACCUMULATION_INTERVAL\n",
    "):\n",
    "    # TODO fix for Roberta model\n",
    "    enc_parameters = [\n",
    "        param for name, param in model.named_parameters()\n",
    "        if param.requires_grad and name.startswith('bert.')\n",
    "    ]\n",
    "    dec_parameters = [\n",
    "        param for name, param in model.named_parameters()\n",
    "        if param.requires_grad and not name.startswith('bert.')\n",
    "    ]\n",
    "    optimizer = AdamW([\n",
    "        dict(params=enc_parameters, lr=enc_lr),\n",
    "        dict(params=dec_parameters, lr=dec_lr),\n",
    "    ], weight_decay=weight_decay)\n",
    "    optimizer.clip_value = clip_value\n",
    "    optimizer.accumulation_interval = accumulation_interval\n",
    "\n",
    "    scheduler = NoamScheduler(optimizer, warmup=warmup)\n",
    "    criter = MSELoss()\n",
    "\n",
    "    return optimizer, scheduler, criter\n",
    "\n",
    "EPOCHS_NUMBER = 5\n",
    "\n",
    "def load_or_train_model(model, device, additional_features):\n",
    "    model_name = f'learn_simple_berta_{additional_features}.pth'.lower()\n",
    "    MODEL_PATH = f'{os.path.expanduser(cfg.weights_path)}/{model_name}'\n",
    "    # ! rm {MODEL_PATH}\n",
    "\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        logging.info(f'Loading model {MODEL_PATH}')\n",
    "        model.load(model_name)\n",
    "        model, device = setup_cuda_device(model)\n",
    "        return model\n",
    "    else:\n",
    "        logging.info('Create dataloaders...')\n",
    "        train_loader, valid_loader = get_dataloaders(\n",
    "            train, val, BATCH_SIZE, ARTICLE_LENGTH, model.tokenizer, additional_features\n",
    "        )\n",
    "\n",
    "        writer = SummaryWriter(log_dir=os.path.expanduser(cfg.log_path))\n",
    "        writer.train_step, writer.eval_step = 0, 0\n",
    "\n",
    "        optimizer, scheduler, criter = prepare_learning_tools(model)\n",
    "\n",
    "        for epoch in range(1, EPOCHS_NUMBER + 1):\n",
    "            logging.info(f\"{epoch} epoch training...\")\n",
    "            model, optimizer, scheduler, writer = train_fun(\n",
    "                model, train_loader, optimizer, scheduler,\n",
    "                criter, device, writer, additional_features\n",
    "            )\n",
    "\n",
    "            logging.info(f\"{epoch} epoch validation...\")\n",
    "            model = evaluate_fun(model, valid_loader, criter, device, writer, additional_features)\n",
    "\n",
    "        logging.info(f'Save trained model to {model_name}')\n",
    "        model.save(model_name)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create and train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ARTICLE_LENGTH = 512\n",
    "\n",
    "model = create_model(\"bert\", \"froze_all\", ARTICLE_LENGTH, additional_features=ADDITIONAL_FEATURES)\n",
    "model, device = setup_cuda_device(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_or_train_model(model, device, additional_features=ADDITIONAL_FEATURES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example of model predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Prepare data for model')\n",
    "ex = val[val['pmid'] == val['pmid'].values[0]]\n",
    "display(ex)\n",
    "\n",
    "print('ex', len(ex))\n",
    "text = ex['sentence'].values\n",
    "print('papers', len(text))\n",
    "\n",
    "print('BERT preprocessing')\n",
    "input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(\n",
    "    text, ARTICLE_LENGTH, model.tokenizer\n",
    ")\n",
    "print('BERT preprocessing done')\n",
    "print('article_ids', len(input_ids))\n",
    "print('attention_mask', len(attention_mask))\n",
    "print('token_type_ids', len(token_type_ids))\n",
    "print('n_sents', n_sents)\n",
    "res_sents = text[:n_sents]\n",
    "scores = ex['score'].values[:n_sents]\n",
    "print('scores', len(scores))\n",
    "\n",
    "if ADDITIONAL_FEATURES:\n",
    "    features = np.nan_to_num(\n",
    "        ex[['sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "            'mean_r_fig', 'mean_r_tab',\n",
    "            'min_r_fig', 'min_r_tab',\n",
    "            'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "    )\n",
    "    features = features[:n_sents]\n",
    "    print('features', len(features))\n",
    "else:\n",
    "    features = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Compute model scores')\n",
    "\n",
    "input_ids = torch.tensor([input_ids]).to(device)\n",
    "attention_mask = torch.tensor([attention_mask]).to(device)\n",
    "token_type_ids = torch.tensor([token_type_ids]).to(device)\n",
    "\n",
    "print('input_ids', input_ids.shape)\n",
    "print('attention_mask', attention_mask.shape)\n",
    "print('token_type_ids', token_type_ids.shape)\n",
    "if ADDITIONAL_FEATURES:\n",
    "    print('additional_features', len(features))\n",
    "    input_features = [torch.tensor(e, dtype=torch.float) for e in features]\n",
    "    input_features = torch.stack(input_features).to(device)\n",
    "    print('input_features', input_features.shape)\n",
    "    draft_probs = model(input_ids, attention_mask, token_type_ids, input_features)\n",
    "else:\n",
    "    draft_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "print('result')\n",
    "to_show_df = pd.DataFrame(\n",
    "    dict(sentence=res_sents, ideal_score=scores / 100, res_score=draft_probs.cpu().detach().numpy())\n",
    ")\n",
    "display(to_show_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate model performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 'model' not in globals():\n",
    "    model = create_model(\"bert\", \"froze_all\", ARTICLE_LENGTH, additional_features=ADDITIONAL_FEATURES)\n",
    "    model, device = setup_cuda_device(model)\n",
    "    model = load_or_train_model(model, device, additional_features=ADDITIONAL_FEATURES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Prepare refs_and_scores dataset')\n",
    "REF_SCORES_PATH = os.path.expanduser(f\"{cfg.base_path}/refs_and_scores.csv\")\n",
    "! rm {REF_SCORES_PATH}\n",
    "\n",
    "if os.path.exists(REF_SCORES_PATH):\n",
    "     final_ref_show_df = pd.read_csv(REF_SCORES_PATH)\n",
    "else:\n",
    "    to_show_ref = pd.merge(train_df, ref_sents_df[['ref_pmid', 'sentence']],\n",
    "                           left_on=['pmid'], right_on=['ref_pmid'])\n",
    "    to_show_ref = to_show_ref.rename(columns=dict(sentence_x='sentence', sentence_y='ref_sentence'))\n",
    "    to_show_ref = to_show_ref[['pmid', 'sentence', 'ref_sentence', 'score']]\n",
    "    final_ref_show_dic = dict(pmid=[], sentence=[], ref_sentence=[], score=[])\n",
    "    ite = [(pmid, sent) for pmid, sent in to_show_ref[['pmid', 'sentence']].values]\n",
    "\n",
    "    for pmid, sent in tqdm(set(ite)):\n",
    "        refs_df = to_show_ref[(to_show_ref['pmid'] == pmid) & (to_show_ref['sentence'] == sent)]\n",
    "        final_ref_show_dic['pmid'].append(pmid)\n",
    "        final_ref_show_dic['sentence'].append(sent)\n",
    "        final_ref_show_dic['ref_sentence'].append(\" \".join(refs_df['ref_sentence'].values))\n",
    "        final_ref_show_dic['score'].append(refs_df['score'].values[0])\n",
    "    final_ref_show_df = pd.DataFrame(final_ref_show_dic)\n",
    "    final_ref_show_df.to_csv(REF_SCORES_PATH, index=False)\n",
    "\n",
    "display(final_ref_show_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Prepare dataset to estimate performance')\n",
    "to_test = final_ref_show_df[final_ref_show_df['pmid'].isin(set(val['pmid'].values))]\n",
    "if ADDITIONAL_FEATURES:\n",
    "    to_test = pd.merge(to_test, train_df[['pmid', 'sentence',\n",
    "                                          'sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "                                          'mean_r_fig', 'mean_r_tab',\n",
    "                                          'min_r_fig', 'min_r_tab',\n",
    "                                          'max_r_fig', 'max_r_tab']],\n",
    "                       left_on=['pmid', 'sentence'], right_on=['pmid', 'sentence'])\n",
    "display(to_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Using model for predictions')\n",
    "res = dict(pmid=[], sentence=[], ref_sentences=[], score=[], res_score=[])\n",
    "\n",
    "for pmid in tqdm(set(to_test['pmid'].values)):\n",
    "    ex = to_test[to_test['pmid'] == pmid]\n",
    "    text = ex['sentence'].values\n",
    "    input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(\n",
    "        text, ARTICLE_LENGTH, model.tokenizer\n",
    "    )\n",
    "    res_sents = text[:n_sents]\n",
    "    scores = ex['score'].values[:n_sents] / 100\n",
    "    input_ids = torch.tensor([input_ids]).to(device)\n",
    "    attention_mask = torch.tensor([attention_mask]).to(device)\n",
    "    token_type_ids = torch.tensor([token_type_ids]).to(device)\n",
    "    if ADDITIONAL_FEATURES:\n",
    "        features = np.nan_to_num(\n",
    "            ex[['sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "                'mean_r_fig', 'mean_r_tab',\n",
    "                'min_r_fig', 'min_r_tab',\n",
    "                'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "        )\n",
    "        features = features[:n_sents]\n",
    "        input_features = [torch.tensor(e, dtype=torch.float) for e in features]\n",
    "        input_features = torch.stack(input_features).to(device)\n",
    "        draft_probs = model(input_ids, attention_mask, token_type_ids, input_features)\n",
    "    else:\n",
    "        draft_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "    for sent, sc, res_sc in zip(res_sents, scores, draft_probs.cpu().detach().numpy()):\n",
    "        res['pmid'].append(pmid)\n",
    "        res['sentence'].append(sent)\n",
    "        res['ref_sentences'].append(ex['ref_sentence'].values[0])\n",
    "        res['score'].append(sc)\n",
    "        res['res_score'].append(res_sc)\n",
    "\n",
    "res_df = pd.DataFrame(res)\n",
    "display(res_df.head())\n",
    "res_df.to_csv(f\"{cfg.base_path}/saved_example_refs.csv\")\n",
    "\n",
    "print('MSE score', ((res_df['score'].values - res_df['res_score'].values) ** 2).mean() ** 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quality analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 'model' not in globals():\n",
    "    model = create_model(\"bert\", \"froze_all\", ARTICLE_LENGTH, additional_features=ADDITIONAL_FEATURES)\n",
    "    model, device = setup_cuda_device(model)\n",
    "    model = load_or_train_model(model, device, additional_features=ADDITIONAL_FEATURES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Searching for review papers')\n",
    "inter = set(sentences_df['pmid'].values) & set(ref_sents_df['ref_pmid'].values)\n",
    "review_papers = list(set(ref_sents_df[ref_sents_df['ref_pmid'].isin(inter)]['pmid'].values))\n",
    "print('Review papers', len(review_papers))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_stat = dict(rev_pmid=[], sent_num=[], rouge=[], true_rouge=[], diff_papers=[])\n",
    "\n",
    "for rev_id in tqdm(review_papers):\n",
    "    paper_ref = sentences_df[sentences_df['pmid'] == rev_id]['sentence'].values\n",
    "    papers_to_check = list(set(ref_sents_df[ref_sents_df['pmid'] == rev_id]['ref_pmid'].values))\n",
    "    result = {'pmid': [], 'sentence': [], 'score': []}\n",
    "    for paper_id in papers_to_check:\n",
    "        ex = test[test['pmid'] == paper_id]\n",
    "        text = ex['sentence'].values\n",
    "\n",
    "        features = np.nan_to_num(\n",
    "            ex[['sent_id', 'sent_type', 'r_abs',\n",
    "                'num_refs', 'mean_r_fig', 'mean_r_tab',\n",
    "                'min_r_fig', 'min_r_tab',\n",
    "                'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "        ) if ADDITIONAL_FEATURES else None\n",
    "        total_sents = 0\n",
    "        while total_sents < len(text):\n",
    "            magic = max(0, total_sents - 5)\n",
    "            input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(\n",
    "                text[magic:], ARTICLE_LENGTH, model.tokenizer\n",
    "            )\n",
    "            if n_sents <= 5:\n",
    "                total_sents += 1\n",
    "                continue\n",
    "            old_total = total_sents\n",
    "            total_sents = magic + n_sents\n",
    "            input_ids = torch.tensor([input_ids]).to(device)\n",
    "            attention_mask = torch.tensor([attention_mask]).to(device)\n",
    "            token_type_ids = torch.tensor([token_type_ids]).to(device)\n",
    "            if ADDITIONAL_FEATURES:\n",
    "                input_features = [torch.tensor(e, dtype=torch.float) for e in features[magic:total_sents]]\n",
    "                input_features = torch.stack(input_features).to(device)\n",
    "                draft_probs = model(input_ids, attention_mask, token_type_ids,  input_features)\n",
    "            else:\n",
    "                draft_probs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            result['pmid'].extend([paper_id] * (total_sents - old_total))\n",
    "            result['sentence'].extend(list(text[old_total:total_sents]))\n",
    "            result['score'].extend(list(draft_probs.cpu().detach().numpy())[old_total - magic:])\n",
    "\n",
    "    res_df = pd.DataFrame(result)\n",
    "    sorted_arr = sorted(list(res_df['score'].values))\n",
    "    for i in range(5, 103, 5):\n",
    "        if len(sorted_arr) < i:\n",
    "            break\n",
    "        threshold = sorted_arr[-i]\n",
    "        final_text = res_df[res_df['score'] >= threshold][['pmid', 'sentence']]\n",
    "        mean_score = 0\n",
    "        num = 0\n",
    "        for sent in final_text['sentence'].values:\n",
    "           for ref_sent in paper_ref:\n",
    "               try:\n",
    "                   mean_score += get_rouge(sent, ref_sent)\n",
    "                   num += 1\n",
    "               except Exception:\n",
    "                   continue\n",
    "        mean_score /= num\n",
    "        real_score = get_rouge(\" \".join(final_text['sentence'].values), \" \".join(paper_ref))\n",
    "        test_stat['rev_pmid'].append(rev_id)\n",
    "        test_stat['sent_num'].append(i)\n",
    "        print(len(\" \".join(final_text['sentence'].values)), len(\" \".join(paper_ref)))\n",
    "\n",
    "        test_stat['rouge'].append(mean_score)\n",
    "        test_stat['true_rouge'].append(real_score)\n",
    "        test_stat['diff_papers'].append(len(set(final_text['pmid'])))\n",
    "\n",
    "test_stat_df = pd.DataFrame(test_stat)\n",
    "test_stat_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(*[len(arr) for key, arr in test_stat.items()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_stat_df.to_csv(f\"{cfg.base_path}/simple_right_test_on_review.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rouge_means = []\n",
    "rouge_err = []\n",
    "papers_means = []\n",
    "papers_err = []\n",
    "\n",
    "for i in range(5, 103, 5):\n",
    "    tmp = test_stat_df.groupby(['sent_num']).get_group(i)\n",
    "    rouge_means.append(tmp['rouge'].mean())\n",
    "    rouge_err.append(tmp['rouge'].std())\n",
    "    papers_means.append(tmp['diff_papers'].mean())\n",
    "    papers_err.append(tmp['diff_papers'].std())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.errorbar(list(range(5, 103, 5)), rouge_means, yerr=rouge_err, fmt='-o')\n",
    "plt.title('Mean rouge value')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.errorbar(list(range(5, 103, 5)), papers_means, yerr=papers_err, fmt='-o')\n",
    "plt.title('Mean number of papers')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: fix comparison with BERTSUM model\n",
    "df_1 = test_stat_df.assign(model = ['Current model']*len(test_stat_df))\n",
    "# df_2 = ft_test_stat_df.assign(model = ['BERTSUM']*len(ft_test_stat_df))\n",
    "draw_df = pd.concat([df_1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.catplot(x=\"sent_num\", y=\"rouge\", kind=\"box\", hue='model', aspect=1.7, color='lightblue',\n",
    "            data=draw_df).set_axis_labels(\"Number of sentences\", \"ROUGE, %\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}