{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training&Evaluation of a developed algorithm\n",
    "\n",
    "This notebook contains source code for several possible architectures to train & evaluate them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from review import config as cfg\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('CUDA version', torch.version.cuda)\n",
    "print('CUDA is available', torch.cuda.is_available())\n",
    "\n",
    "\n",
    "def setup_cuda_device(model):\n",
    "    logging.info('Setup single-device settings...')\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    return model, device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logging.info('Fix seed')\n",
    "seed = cfg.seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading data and preparation of dataset\n",
    "\n",
    "`load_data` loads all the needed datafiles for building train dataset.\\\n",
    "The several next steps are only should be done if no train/test/val datasets are saved."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def load_data(additional_features):\n",
    "    dataset_root = os.path.expanduser(cfg.dataset_path)\n",
    "    logging.info(f'Loading references dataset from {dataset_root}')\n",
    "\n",
    "    logging.info('Loading citations_df')\n",
    "    citations_df = pd.read_csv(os.path.join(dataset_root, \"citations.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(citations_df)))\n",
    "\n",
    "    logging.info('Loading sentences_df')\n",
    "    sentences_df = pd.read_csv(os.path.join(dataset_root, \"sentences.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(sentences_df)))\n",
    "\n",
    "    logging.info('Loading review_files_df')\n",
    "    review_files_df = pd.read_csv(os.path.join(dataset_root, \"review_files.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(review_files_df)))\n",
    "\n",
    "    logging.info('Loading reverse_ref_df')\n",
    "    reverse_ref_df = pd.read_csv(os.path.join(dataset_root, \"reverse_ref.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(reverse_ref_df)))\n",
    "\n",
    "    if not additional_features:\n",
    "        return citations_df, sentences_df, review_files_df, reverse_ref_df, None, None, None\n",
    "\n",
    "    logging.info('Loading abstracts_df')\n",
    "    abstracts_df = pd.read_csv(os.path.join(dataset_root, \"abstracts.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(abstracts_df)))\n",
    "\n",
    "    logging.info('Loading figures_df')\n",
    "    figures_df = pd.read_csv(os.path.join(dataset_root, \"figures.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(figures_df)))\n",
    "\n",
    "    logging.info('Loading tables_df')\n",
    "    tables_df = pd.read_csv(os.path.join(dataset_root, \"tables.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(tables_df)))\n",
    "\n",
    "    return citations_df, sentences_df, review_files_df, reverse_ref_df, abstracts_df, figures_df, tables_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ADDITIONAL_FEATURES = False\n",
    "\n",
    "citations_df, sentences_df, review_files_df, reverse_ref_df, abstracts_df, figures_df, tables_df = load_data(\n",
    "    additional_features=ADDITIONAL_FEATURES)\n",
    "\n",
    "logging.info('Done loading references dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "res = Counter(list(sentences_df['pmid'].values))\n",
    "plt.hist(res.values(), bins=range(-1, 400))\n",
    "plt.title('Length of papers')\n",
    "plt.show()\n",
    "del res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "REF_SENTS_DF_PATH = f\"{os.path.expanduser(cfg.base_path)}/ref_sents.csv\"\n",
    "! rm {REF_SENTS_DF_PATH}\n",
    "\n",
    "if os.path.exists(REF_SENTS_DF_PATH):\n",
    "    ref_sents_df = pd.read_csv(REF_SENTS_DF_PATH, sep='\\t')\n",
    "else:\n",
    "    logging.info('Creating reference sentences dataset')\n",
    "    ref_sents_df = pd.merge(citations_df, reverse_ref_df, left_on=['pmid', 'ref_id'], right_on=['pmid', 'ref_id'])\n",
    "    ref_sents_df = pd.merge(ref_sents_df, sentences_df, left_on=['pmid', 'sent_type', 'sent_id'],\n",
    "                            right_on=['pmid', 'type', 'sent_id'])\n",
    "    ref_sents_df = ref_sents_df[ref_sents_df['pmid'].isin(review_files_df['pmid'].values)]\n",
    "    ref_sents_df = ref_sents_df.drop_duplicates()\n",
    "    logging.info(f'Len of unique ref_sents {len(set(ref_sents_df[\"ref_pmid\"]))}')\n",
    "    ref_sents_df = ref_sents_df[['pmid', 'ref_id', 'pub_type', 'ref_pmid', 'sent_type', 'sent_id', 'sentence']]\n",
    "    ref_sents_df.to_csv(REF_SENTS_DF_PATH, sep='\\t', index=False)\n",
    "\n",
    "logging.info('Cleanup memory')\n",
    "del citations_df\n",
    "del review_files_df\n",
    "\n",
    "display(ref_sents_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rouge\n",
    "`get_rouge` function allows to compute similarity between two sentences.\n",
    "`rouge-l` is another possible option."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "ROUGE_METER = Rouge()\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "def get_rouge(sent1, sent2):\n",
    "    if sent1 is None or sent2 is None:\n",
    "        return None\n",
    "    sent_1 = TOKENIZER.tokenize(sent1)\n",
    "    if len(sent_1) == 0:\n",
    "        return None\n",
    "    sent_1 = \" \".join(list(filter(lambda x: x.isalpha() or x in '.!,?', sent_1)))\n",
    "    sent_2 = TOKENIZER.tokenize(sent2)\n",
    "    if len(sent_2) == 0:\n",
    "        return None\n",
    "    sent_2 = \" \".join(list(filter(lambda x: x.isalpha() or x in '.!,?', sent_2)))\n",
    "    if len(sent_1) == 0 or len(sent_2) == 0:\n",
    "        return None\n",
    "    rouges = ROUGE_METER.get_scores(sent_1, sent_2)[0]\n",
    "    rouges = [rouges[f'rouge-{x}'][\"f\"] for x in ('1', '2')]  # , 'l')]\n",
    "    return np.mean(rouges) * 100\n",
    "\n",
    "\n",
    "def mean_rouge(sent, text):\n",
    "    if len(text) == 0:\n",
    "        return None\n",
    "    try:\n",
    "        return sum(get_rouge(sent, ref_sent) for ref_sent in text) / len(text)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception at mean_rouge {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def min_rouge(sent, text):\n",
    "    try:\n",
    "        score = 100000000\n",
    "        for ref_sent in text:\n",
    "            score = min(get_rouge(sent, ref_sent), score)\n",
    "        if score == 100000000:\n",
    "            return None\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception at min_rouge {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def max_rouge(sent, text):\n",
    "    try:\n",
    "        score = -100000\n",
    "        for ref_sent in text:\n",
    "            score = max(get_rouge(sent, ref_sent), score)\n",
    "        if score == -100000:\n",
    "            return None\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception at max_rouge {e}')\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_rouge(\"I am scout. True!\", \"No you are not a scout.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build train / test/ validate datasets\n",
    "\n",
    "To create a dataset with features use `preprocess_paper_with_features`. Otherwise, use `preprocess_paper`.\n",
    "\n",
    "In case datasets are not yet created and `ref_sents_df` is also not yet created, let's create `ref_sents_df`.\\\n",
    "For each paper pmid there is a list of sentences from review papers in which the paper with this `pmid` is cited."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "REPLACE_SYMBOLS = {\n",
    "    '—': '-',\n",
    "    '–': '-',\n",
    "    '―': '-',\n",
    "    '…': '...',\n",
    "    '´´': \"´\",\n",
    "    '´´´': \"´´\",\n",
    "    \"''\": \"'\",\n",
    "    \"'''\": \"'\",\n",
    "    \"``\": \"`\",\n",
    "    \"```\": \"`\",\n",
    "    \":\": \" : \",\n",
    "}\n",
    "\n",
    "\n",
    "def parse_sents(data):\n",
    "    sents = sum([sent_tokenize(text) for text in data], [])\n",
    "    sents = list(filter(lambda x: len(x) > 3, sents))\n",
    "    return sents\n",
    "\n",
    "\n",
    "def sent_standardize(sent):\n",
    "    sent = unidecode(sent)\n",
    "    sent = re.sub(r\"\\[(xref_\\w*_\\w\\d*]*)(, xref_\\w*_\\w\\d*)*\\]\", \" \", sent)  # delete [xref,...]\n",
    "    sent = re.sub(r\"\\( (xref_\\w*_\\w\\d*)(; xref_\\w*_\\w\\d*)* \\)\", \" \", sent)  # delete (xref; ...)\n",
    "    sent = re.sub(r\"\\[xref_\\w*_\\w\\d*\\]\", \" \", sent)  # delete [xref]\n",
    "    sent = re.sub(r\"xref_\\w*_\\w\\d*\", \" \", sent)  # delete [[xref]]\n",
    "    for k, v in REPLACE_SYMBOLS.items():\n",
    "        sent = sent.replace(k, v)\n",
    "    return sent.strip()\n",
    "\n",
    "\n",
    "def standardize(text):\n",
    "    return [x for x in (sent_standardize(sent) for sent in text) if len(x) > 3]\n",
    "\n",
    "\n",
    "PAPER_MIN_SENTENCES = 50\n",
    "PAPER_MAX_SENTENCES = 100\n",
    "\n",
    "\n",
    "def preprocess_paper(\n",
    "        paper_id, sentences_df, ref_sents_df,\n",
    "        paper_min_sents=PAPER_MIN_SENTENCES, paper_max_sents=PAPER_MAX_SENTENCES):\n",
    "    paper = sentences_df[sentences_df['pmid'] == paper_id]['sentence']\n",
    "    paper = standardize(paper)\n",
    "\n",
    "    ref_sents = ref_sents_df[ref_sents_df['ref_pmid'] == paper_id]['sentence']\n",
    "    ref_sents = standardize(ref_sents)\n",
    "\n",
    "    if len(paper) < paper_min_sents:\n",
    "        return None\n",
    "\n",
    "    if len(paper) > paper_max_sents:\n",
    "        paper = list(paper[:paper_min_sents]) + list(paper[-paper_min_sents:])\n",
    "\n",
    "    preprocessed_score = [\n",
    "        sum(get_rouge(sent, ref_sent) for ref_sent in ref_sents) / len(ref_sents) for sent in paper\n",
    "    ]\n",
    "    return paper, preprocessed_score\n",
    "\n",
    "\n",
    "def preprocess_paper_with_features(paper_id, sentences_df, ref_sents_df,\n",
    "                                   abstracts_df, figures_df, reverse_ref_df, tables_df,\n",
    "                                   paper_min_sents=PAPER_MIN_SENTENCES, paper_max_sents=PAPER_MAX_SENTENCES):\n",
    "    preprocessed_score = []\n",
    "    features = []\n",
    "\n",
    "    papers = sentences_df[sentences_df['pmid'] == paper_id]['sentence']\n",
    "    papers = standardize(papers)\n",
    "\n",
    "    sent_ids = sentences_df[sentences_df['pmid'] == paper_id]['sent_id']\n",
    "    sent_types = sentences_df[sentences_df['pmid'] == paper_id]['type']\n",
    "\n",
    "    ref_sents = ref_sents_df[ref_sents_df['ref_pmid'] == paper_id]['sentence']\n",
    "    ref_sents = standardize(ref_sents)\n",
    "\n",
    "    fig_captions = figures_df[figures_df['pmid'] == paper_id]['caption']\n",
    "    fig_captions = standardize(fig_captions)\n",
    "\n",
    "    tab_captions = tables_df[tables_df['pmid'] == paper_id]['caption']\n",
    "    tab_captions = standardize(tab_captions)\n",
    "\n",
    "    abstract = abstracts_df[abstracts_df['pmid'] == paper_id]['abstract']\n",
    "    if len(abstract) != 0:\n",
    "        abstract = standardize(abstract)\n",
    "\n",
    "    tmp_df = reverse_ref_df[reverse_ref_df['pmid'] == paper_id]\n",
    "\n",
    "    if len(papers) < paper_min_sents:\n",
    "        return None\n",
    "\n",
    "    if len(papers) > paper_max_sents:\n",
    "        papers = list(papers[:paper_min_sents]) + list(papers[-paper_min_sents:])\n",
    "        sent_ids = list(sent_ids[:paper_min_sents]) + list(sent_ids[-paper_min_sents:])\n",
    "        sent_types = list(sent_types[:paper_min_sents]) + list(sent_types[-paper_min_sents:])\n",
    "\n",
    "    for sent, sent_type, sent_id in zip(papers, sent_types, sent_ids):\n",
    "        score = mean_rouge(sent, ref_sents)\n",
    "        if score is None:\n",
    "            return None\n",
    "\n",
    "        r_abs = get_rouge(sent, abstract[0])\n",
    "        num_refs = len(tmp_df[(tmp_df['sent_type'] == sent_type) & (tmp_df['sent_id'] == sent_id)])\n",
    "        preprocessed_score.append(score)\n",
    "        features.append((sent_id, int(sent_type == \"general\"), r_abs, num_refs,\n",
    "                         mean_rouge(sent, fig_captions), mean_rouge(sent, tab_captions),\n",
    "                         min_rouge(sent, fig_captions), min_rouge(sent, tab_captions),\n",
    "                         max_rouge(sent, fig_captions), max_rouge(sent, tab_captions)))\n",
    "    return papers, preprocessed_score, features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FEATURES_NUMBER = 10\n",
    "\n",
    "\n",
    "def process_reference_sentences_dataset(sentences_df, ref_sents_df, additional_features):\n",
    "    res = {}\n",
    "    inter = set(sentences_df['pmid'].values) & set(ref_sents_df['ref_pmid'].values)\n",
    "    for pmid in tqdm(inter):\n",
    "        try:\n",
    "            if additional_features:\n",
    "                temp = preprocess_paper_with_features(\n",
    "                    pmid, sentences_df, ref_sents_df, abstracts_df, figures_df, reverse_ref_df, tables_df\n",
    "                )\n",
    "            else:\n",
    "                temp = preprocess_paper(pmid, sentences_df, ref_sents_df)\n",
    "        except Exception as e:\n",
    "            logging.warning(f'Error during processing {pmid} {e}')\n",
    "            continue\n",
    "        if temp is None:\n",
    "            logging.warning(f'temp is None for {pmid}')\n",
    "            continue\n",
    "        res[pmid] = temp\n",
    "        print(f\"\\r{pmid} {np.mean(res[pmid][1])}\", end=\"\")\n",
    "\n",
    "    logging.info(f'Successfully preprocessed {len(res)} of {len(inter)} papers')\n",
    "\n",
    "    logging.info(f'Creating train dataset')\n",
    "    feature_names = [\n",
    "        'sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "        'mean_r_fig', 'mean_r_tab', 'min_r_fig', 'min_r_tab', 'max_r_fig', 'max_r_tab'\n",
    "    ]\n",
    "    assert len(feature_names) == FEATURES_NUMBER\n",
    "    train_dic = dict(\n",
    "        pmid=[], sentence=[], score=[], sent_id=[], sent_type=[], r_abs=[], num_refs=[],\n",
    "        mean_r_fig=[], mean_r_tab=[], min_r_fig=[], min_r_tab=[], max_r_fig=[], max_r_tab=[]\n",
    "    )\n",
    "\n",
    "    for pmid, stat in tqdm(res.items()):\n",
    "        if len(stat) == 2:\n",
    "            for sent, score in zip(*stat):\n",
    "                train_dic['pmid'].append(pmid)\n",
    "                train_dic['sentence'].append(sent)\n",
    "                train_dic['score'].append(score)\n",
    "        else:\n",
    "            for sent, score, features in zip(*stat):\n",
    "                train_dic['pmid'].append(pmid)\n",
    "                train_dic['sentence'].append(sent)\n",
    "                train_dic['score'].append(score)\n",
    "                for name, val in zip(feature_names, features):\n",
    "                    train_dic[name].append(val)\n",
    "\n",
    "    train_df = pd.DataFrame({k: v for k, v in train_dic.items() if v})\n",
    "    logging.info(f'Full train dataset {len(train_df)}')\n",
    "    return train_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = f'{os.path.expanduser(cfg.base_path)}/dataset.csv'\n",
    "! rm {TRAIN_DATASET_PATH}\n",
    "\n",
    "if os.path.exists(TRAIN_DATASET_PATH):\n",
    "    train_df = pd.read_csv(TRAIN_DATASET_PATH)\n",
    "else:\n",
    "    train_df = process_reference_sentences_dataset(\n",
    "        sentences_df, ref_sents_df, additional_features=ADDITIONAL_FEATURES\n",
    "    )\n",
    "    train_df.to_csv(TRAIN_DATASET_PATH, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(train_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "res = Counter(list(train_df['score'].values))\n",
    "\n",
    "plt.hist(res.values(), bins=range(2, 20))\n",
    "plt.title('Train dataset scores')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Splitting data into train/test/val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(list(set(train_df['pmid'].values)), test_size=0.2)\n",
    "test_ids, val_ids = train_test_split(test_ids, test_size=0.4)\n",
    "\n",
    "train = train_df[train_df['pmid'].isin(train_ids)]\n",
    "logging.info(f'Train {len(train)}')\n",
    "display(train.head(1))\n",
    "\n",
    "test = train_df[train_df['pmid'].isin(test_ids)]\n",
    "logging.info(f'Test {len(test)}')\n",
    "display(test.head(1))\n",
    "\n",
    "val = train_df[train_df['pmid'].isin(val_ids)]\n",
    "logging.info(f'Validate {len(val)}')\n",
    "display(val.head(1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inspect pretrained models: BERT and Roberta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel, RobertaModel\n",
    "\n",
    "backbone = BertModel.from_pretrained(\n",
    "    \"bert-base-uncased\", output_hidden_states=False\n",
    ")\n",
    "print('BERT pretrained')\n",
    "print(f'Parameters {sum(p.numel() for p in backbone.parameters() if p.requires_grad)}')\n",
    "print(', '.join(n for n, p in backbone.named_parameters()))\n",
    "print(backbone)\n",
    "\n",
    "# backbone = RobertaModel.from_pretrained(\n",
    "#     'roberta-base', output_hidden_states=False\n",
    "# )\n",
    "# print('ROBERTA pretrained')\n",
    "# print(f'Parameters {sum(p.numel() for p in backbone.parameters() if p.requires_grad)}')\n",
    "# print(', '.join(n for n, p in backbone.named_parameters()))\n",
    "# print(backbone)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing text for BERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_paper_bert(text, max_len, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocess text for Bert / Robert model.\n",
    "    NOTE: not all the text can be processed because of max_len.\n",
    "    :param text: list(list(str))\n",
    "    :param max_len: maximum length of preprocessing\n",
    "    :param tokenizer: Bert of Robert tokenize\n",
    "    :return:\n",
    "        ids | tokenized ids of length max_len, 0 if padding\n",
    "        attention_mask | list(str) 1 if real token, not padding\n",
    "        token_type_ids | 0-1 for different sentences\n",
    "        n_sents | number of actual sentences encoded\n",
    "    \"\"\"\n",
    "    sents = [\n",
    "        [tokenizer.artBOS.tkn] + tokenizer.tokenize(sent) + [tokenizer.artEOS.tkn] for sent in text\n",
    "    ]\n",
    "    logging.debug(f'sents {sents}')\n",
    "    ids, token_type_ids, segment_signature = [], [], 0\n",
    "    n_sents = 0\n",
    "    for i, s in enumerate(sents):\n",
    "        logging.debug(f'sentence {i} {s}')\n",
    "        logging.debug(f'ids {len(ids)}')\n",
    "        logging.debug(f'segments {len(token_type_ids)}')\n",
    "        logging.debug(f'segment_signature {segment_signature}')\n",
    "        if len(ids) + len(s) <= max_len:\n",
    "            n_sents += 1\n",
    "            ids.extend(tokenizer.convert_tokens_to_ids(s))\n",
    "            token_type_ids.extend([segment_signature] * len(s))\n",
    "            segment_signature = (segment_signature + 1) % 2\n",
    "        else:\n",
    "            logging.debug(f'break, len(s)={len(s)}')\n",
    "            break\n",
    "    attention_mask = [1] * len(ids)\n",
    "\n",
    "    logging.debug('Padding data')\n",
    "    pad_len = max(0, max_len - len(ids))\n",
    "    ids += [tokenizer.PAD.idx] * pad_len\n",
    "    attention_mask += [0] * pad_len\n",
    "    token_type_ids += [segment_signature] * pad_len\n",
    "    assert len(ids) == len(attention_mask)\n",
    "    assert len(ids) == len(token_type_ids)\n",
    "    return ids, attention_mask, token_type_ids, n_sents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Inspect preprocessing text for Bert')\n",
    "\n",
    "tokenizer = Summarizer.initialize_bert_tokenizer()\n",
    "text = list(train_df.head(5)['sentence'])\n",
    "input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(text, 512, tokenizer)\n",
    "\n",
    "print(f'input_ids {input_ids}')\n",
    "print(f'attention_mask {attention_mask}')\n",
    "print(f'token_type_ids {token_type_ids}')\n",
    "print(f'n_sents {n_sents}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main model classes\n",
    "\n",
    "The model in main pubtrends application is loaded using `load_model` function from `review.model` module.\n",
    "\n",
    "It has several options to set up:\n",
    "* with or without features (right now without features works better),\n",
    "* `BERT` or `roberta` as basis (no big difference),\n",
    "\n",
    "You can also choose `frozen_strategy`:\n",
    "* `froze_all` in case you don't want to improve bert layers but only the summarization layer,\n",
    "* `unfroze_last` -- modifies bert weights and still training not very slow,\n",
    "* `unfroze_all` -- the training is slow, the results may better though"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from transformers import BertModel, RobertaModel\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "\n",
    "import review.config as cfg\n",
    "\n",
    "SpecToken = namedtuple('SpecToken', ['tkn', 'idx'])\n",
    "ConvertToken2Id = lambda tokenizer, tkn: tokenizer.convert_tokens_to_ids([tkn])[0]\n",
    "\n",
    "EMBEDDINGS_ADDITIONAL = 100\n",
    "\n",
    "FEATURES_NN_INTERMEDIATE = 100\n",
    "FEATURES_NN_OUT = 50\n",
    "FEATURES_DROPOUT = 0.1\n",
    "\n",
    "BERT_PARAMS_NOT_INITIALIZED = {\n",
    "    'cls.seq_relationship.weight', 'cls.seq_relationship.bias',\n",
    "    'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias',\n",
    "    'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias',\n",
    "    'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight'\n",
    "}\n",
    "\n",
    "\n",
    "class Summarizer(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the main summarization model.\n",
    "    See https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_tf_bert.py\n",
    "    It operates the same input format as original BERT used underneath.\n",
    "    See forward, evaluate params description.\n",
    "    \"\"\"\n",
    "    enc_output: torch.Tensor\n",
    "    dec_ids_mask: torch.Tensor\n",
    "    encdec_ids_mask: torch.Tensor\n",
    "\n",
    "    def __init__(self, model_type, article_len, additional_features, num_features=FEATURES_NUMBER):\n",
    "        super(Summarizer, self).__init__()\n",
    "\n",
    "        print(f'Initialize backbone and tokenizer for {model_type}')\n",
    "        self.article_len = article_len\n",
    "        if model_type == 'bert':\n",
    "            self.backbone = self.initialize_bert()\n",
    "            self.tokenizer = self.initialize_bert_tokenizer()\n",
    "        elif model_type == 'roberta':\n",
    "            self.backbone = self.initialize_roberta()\n",
    "            self.tokenizer = self.initialize_roberta_tokenizer()\n",
    "        else:\n",
    "            raise Exception(f\"Wrong model_type argument: {model_type}\")\n",
    "        self.backbone.resize_token_embeddings(EMBEDDINGS_ADDITIONAL + self.tokenizer.vocab_size)\n",
    "\n",
    "        if additional_features:\n",
    "            print('Adding additional features double fully connected nn')\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Linear(num_features, FEATURES_NN_INTERMEDIATE),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(FEATURES_DROPOUT),\n",
    "                nn.Linear(FEATURES_NN_INTERMEDIATE, FEATURES_NN_OUT)\n",
    "            )\n",
    "        else:\n",
    "            self.features = None\n",
    "\n",
    "        print('Initialize backbone embeddings pulling')\n",
    "\n",
    "        def backbone_forward(input_ids, attention_mask, token_type_ids, position_ids):\n",
    "            return self.backbone(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids\n",
    "            )\n",
    "\n",
    "        self.encoder = lambda *args: backbone_forward(*args)[0]\n",
    "\n",
    "        print('Initialize decoder')\n",
    "        if additional_features:\n",
    "            self.decoder = Classifier(cfg.d_hidden + FEATURES_NN_OUT)\n",
    "        else:\n",
    "            self.decoder = Classifier(cfg.d_hidden)\n",
    "\n",
    "    def expand_positional_embs_if_need(self):\n",
    "        print('Expand positional embeddings if need')\n",
    "        print('Positional embeddings', self.backbone.config.max_position_embeddings, self.article_len)\n",
    "        if self.article_len > self.backbone.config.max_position_embeddings:\n",
    "            old_maxlen = self.backbone.config.max_position_embeddings\n",
    "            old_w = self.backbone.embeddings.position_embeddings.weight\n",
    "            logging.info(f'Backbone pos embeddings expanded from {old_maxlen} upto {self.article_len}')\n",
    "            self.backbone.embeddings.position_embeddings = nn.Embedding(\n",
    "                self.article_len, self.backbone.config.hidden_size\n",
    "            )\n",
    "            self.backbone.embeddings.position_embeddings.weight[:old_maxlen].data.copy_(old_w)\n",
    "            self.backbone.config.max_position_embeddings = self.article_len\n",
    "            print('New positional embeddings', self.backbone.config.max_position_embeddings)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_bert():\n",
    "        return BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", output_hidden_states=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_bert_tokenizer():\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        BOS = \"[CLS]\"\n",
    "        EOS = \"[SEP]\"\n",
    "        PAD = \"[PAD]\"\n",
    "        Summarizer._init_tokenizer(tokenizer, BOS, EOS, PAD)\n",
    "        return tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_roberta():\n",
    "        backbone = RobertaModel.from_pretrained(\n",
    "            'roberta-base', output_hidden_states=False\n",
    "        )\n",
    "        print('initialize token type emb, by default roberta doesnt have it')\n",
    "        backbone.config.type_vocab_size = 2\n",
    "        backbone.embeddings.token_type_embeddings = nn.Embedding(2, backbone.config.hidden_size)\n",
    "        backbone.embeddings.token_type_embeddings.weight.data.normal_(\n",
    "            mean=0.0, std=backbone.config.initializer_range\n",
    "        )\n",
    "        return backbone\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_roberta_tokenizer():\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "        BOS = \"<s>\"\n",
    "        EOS = \"</s>\"\n",
    "        PAD = \"<pad>\"\n",
    "        Summarizer._init_tokenizer(tokenizer, BOS, EOS, PAD)\n",
    "        return tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_tokenizer(tokenizer, BOS, EOS, PAD):\n",
    "        print('Initializing tokenizer with special tokens')\n",
    "        PAD = SpecToken(PAD, ConvertToken2Id(tokenizer, PAD))\n",
    "        artBOS = SpecToken(BOS, ConvertToken2Id(tokenizer, BOS))\n",
    "        artEOS = SpecToken(EOS, ConvertToken2Id(tokenizer, EOS))\n",
    "        print('Add special tokens to tokenizer')\n",
    "\n",
    "        tokenizer.add_special_tokens(dict(additional_special_tokens=[\"<sum>\", \"</sent>\", \"</sum>\"]))\n",
    "        sumBOS = SpecToken(\"<sum>\", ConvertToken2Id(tokenizer, \"<sum>\"))\n",
    "        sumEOS = SpecToken(\"</sent>\", ConvertToken2Id(tokenizer, \"</sent>\"))\n",
    "        sumEOA = SpecToken(\"</sum>\", ConvertToken2Id(tokenizer, \"</sum>\"))\n",
    "\n",
    "        print('Configure tokenizer')\n",
    "        tokenizer.PAD = PAD\n",
    "        tokenizer.artBOS = artBOS\n",
    "        tokenizer.artEOS = artEOS\n",
    "        tokenizer.sumBOS = sumBOS\n",
    "        tokenizer.sumEOS = sumEOS\n",
    "        tokenizer.sumEOA = sumEOA\n",
    "        print('Done initializing tokenizer with special tokens')\n",
    "\n",
    "    def save(self, save_filename):\n",
    "        \"\"\" Save model in filename\n",
    "\n",
    "        :param save_filename: str\n",
    "        \"\"\"\n",
    "        state = dict(\n",
    "            encoder_dict=self.backbone.state_dict(),\n",
    "            decoder_dict=self.decoder.state_dict()\n",
    "        )\n",
    "        if self.features:\n",
    "            state['features_dict'] = self.features.state_dict()\n",
    "        models_folder = os.path.expanduser(cfg.weights_path)\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.makedirs(models_folder)\n",
    "        torch.save(state, f\"{models_folder}/{save_filename}.pth\")\n",
    "\n",
    "    def load(self, load_filename):\n",
    "        path = f\"{os.path.expanduser(cfg.weights_path)}/{load_filename}.pth\"\n",
    "        state = torch.load(path, map_location=lambda storage, location: storage)\n",
    "        self.backbone.load_state_dict(state['encoder_dict'])\n",
    "        self.decoder.load_state_dict(state['decoder_dict'])\n",
    "        if self.features:\n",
    "            self.features.load_state_dict(state['features_dict'])\n",
    "\n",
    "    def froze_backbone(self, froze_strategy):\n",
    "\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            param.requires_grad_(name in BERT_PARAMS_NOT_INITIALIZED)\n",
    "        assert froze_strategy in ['froze_all', 'unfroze_last',\n",
    "                                  'unfroze_all'], f\"incorrect froze_strategy argument: {froze_strategy}\"\n",
    "\n",
    "        if froze_strategy == 'froze_all':\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        elif froze_strategy == 'unfroze_last':\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                param.requires_grad_(\n",
    "                    'encoder.layer.11' in name or\n",
    "                    'encoder.layer.10' in name or\n",
    "                    'encoder.layer.9' in name\n",
    "                )\n",
    "\n",
    "        elif froze_strategy == 'unfroze_all':\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad_(True)\n",
    "\n",
    "    def unfroze_head(self):\n",
    "        for param in self.decoder.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, input_features=None):\n",
    "        \"\"\"\n",
    "        :param input_ids: torch.Size([batch_size, article_len])\n",
    "        Indices of input sequence tokens in the vocabulary.\n",
    "        :param attention_mask: torch.Size([batch_size, article_len])\n",
    "        Mask to avoid performing attention on padding token indices.\n",
    "        Mask values selected in `[0, 1]`:\n",
    "        - 1 for tokens that are **not masked**,\n",
    "        - 0 for tokens that are **masked**.\n",
    "        :param token_type_ids: torch.Size([batch_size, article_len])\n",
    "        Segment token indices to indicate first and second portions of the inputs.\n",
    "        Indices are selected in `[0, 1]`:\n",
    "        - 0 corresponds to a *sentence A* token,\n",
    "        - 1 corresponds to a *sentence B* token.\n",
    "        :return: scores | torch.Size([batch_size, summary_len])\n",
    "        \"\"\"\n",
    "        cls_mask = (input_ids == self.tokenizer.artBOS.idx)\n",
    "\n",
    "        # Indices of positions of each input sequence tokens in the position embeddings.\n",
    "        # position ids | torch.Size([batch_size, article_len])\n",
    "        pos_ids = torch.arange(\n",
    "            0,\n",
    "            self.article_len,\n",
    "            dtype=torch.long,\n",
    "            device=input_ids.device\n",
    "        ).unsqueeze(0).repeat(len(input_ids), 1)\n",
    "\n",
    "        # extract bert embeddings | torch.Size([batch_size, article_len, d_bert])\n",
    "        enc_output = self.encoder(input_ids, attention_mask, token_type_ids, pos_ids)\n",
    "\n",
    "        if self.features:\n",
    "            out_features = self.features(input_features)\n",
    "            scores = self.decoder(torch.cat([enc_output[cls_mask], out_features], dim=-1))\n",
    "        else:\n",
    "            scores = self.decoder(enc_output[cls_mask])\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def evaluate(self, input_ids, attention_mask, token_type_ids, input_features=None):\n",
    "        \"\"\"See forward for parameters and output description\"\"\"\n",
    "\n",
    "        cls_mask = (input_ids == self.tokenizer.artBOS.idx)\n",
    "\n",
    "        # position ids | torch.Size([batch_size, article_len])\n",
    "        pos_ids = torch.arange(\n",
    "            0,\n",
    "            self.article_len,\n",
    "            dtype=torch.long,\n",
    "            device=input_ids.device\n",
    "        ).unsqueeze(0).repeat(len(input_ids), 1)\n",
    "\n",
    "        # extract bert embeddings | torch.Size([batch_size, article_len, d_bert])\n",
    "        enc_output = self.encoder(input_ids, attention_mask, token_type_ids, pos_ids)\n",
    "\n",
    "        scores = []\n",
    "        for eo, cm in zip(enc_output, cls_mask):\n",
    "            if self.features:\n",
    "                out_features = self.features(input_features)\n",
    "                score = self.decoder.evaluate(torch.cat([eo[cm], out_features], dim=-1))\n",
    "            else:\n",
    "                score = self.decoder.evaluate(eo[cm])\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x).squeeze(-1))\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        return self.sigmoid(self.linear(x).squeeze(-1))\n",
    "\n",
    "\n",
    "def create_model(model_type, froze_strategy, article_len, additional_features):\n",
    "    model = Summarizer(model_type, article_len, additional_features)\n",
    "    model.expand_positional_embs_if_need()\n",
    "    # Load intermediate model\n",
    "    #     model.load('temp')\n",
    "    model.froze_backbone(froze_strategy)\n",
    "    model.unfroze_head()\n",
    "    if additional_features:\n",
    "        print('Parameters for features NN', sum(p.numel() for p in model.features.parameters() if p.requires_grad))\n",
    "    print('Parameters for backbone', sum(p.numel() for p in model.backbone.parameters() if p.requires_grad))\n",
    "    print('Parameters for classifier', sum(p.numel() for p in model.decoder.parameters() if p.requires_grad))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and evaluate functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "\n",
    "\n",
    "def get_enc_lr(optimizer):\n",
    "    return optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "def get_dec_lr(optimizer):\n",
    "    return optimizer.param_groups[1]['lr']\n",
    "\n",
    "\n",
    "def backward_step(loss: torch.Tensor, optimizer: Optimizer, model: nn.Module, clip: float):\n",
    "    loss.backward()\n",
    "    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    return total_norm\n",
    "\n",
    "def batch_to_device(batch, additional_features, device):\n",
    "    batch_on_device = [(x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch]\n",
    "    if additional_features:\n",
    "        input_ids, attention_mask, token_type_ids, target_scores, input_features = batch_on_device\n",
    "        input_features = torch.cat(input_features).to(device)\n",
    "    else:\n",
    "        input_ids, attention_mask, token_type_ids, target_scores = batch_on_device\n",
    "        input_features = None\n",
    "    return input_ids, attention_mask, token_type_ids, target_scores, input_features\n",
    "\n",
    "def train_fun(\n",
    "        model,\n",
    "        dataloader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        criter,\n",
    "        device,\n",
    "        writer,\n",
    "        additional_features\n",
    "):\n",
    "    # Put the model into training mode. Don't be mislead--the call to\n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    target_val = 0\n",
    "    mean_sents = 0\n",
    "    szs = 0\n",
    "\n",
    "    for idx_batch, batch in enumerate(dataloader):\n",
    "        input_ids, attention_mask, token_type_ids, target_scores, input_features = \\\n",
    "            batch_to_device(batch, additional_features, device)\n",
    "\n",
    "        sizes = [dc.shape[0] for dc in target_scores]\n",
    "        mean_sents += sum(sizes)\n",
    "        szs += len(sizes)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because\n",
    "        # accumulating the gradients is \"convenient while training RNNs\".\n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        logging.debug(f'batch {idx_batch}')\n",
    "        logging.debug('forward')\n",
    "        logging.debug(f'input_ids {input_ids.shape}')\n",
    "        logging.debug(f'attention_mask {attention_mask.shape}')\n",
    "        logging.debug(f'token_type_ids {token_type_ids.shape}')\n",
    "        if additional_features:\n",
    "            logging.debug(f'input_features {input_features.shape}')\n",
    "\n",
    "        model_scores = model(input_ids, attention_mask, token_type_ids, input_features)\n",
    "        logging.debug(f'models_scores {model_scores.shape}')\n",
    "        logging.debug(f'target_scores {len(target_scores)}')\n",
    "\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "        target_val += sum(target_scores) / len(target_scores)\n",
    "        try:\n",
    "            # loss\n",
    "            loss = criter(model_scores, target_scores, )\n",
    "            loss_val += loss.item()\n",
    "            logging.debug(f'loss {loss}')\n",
    "        except Exception:\n",
    "            print(idx_batch, model_scores.shape, target_scores.shape, token_type_ids)\n",
    "            return\n",
    "\n",
    "        # backward\n",
    "        logging.debug('backward')\n",
    "        grad_norm = backward_step(loss, optimizer, model, optimizer.clip_value)\n",
    "        grad_norm = 0 if (math.isinf(grad_norm) or math.isnan(grad_norm)) else grad_norm\n",
    "\n",
    "        # record a loss value\n",
    "        logging.debug(f'{idx_batch} / {len(dataloader)} train loss {loss.item()}')\n",
    "        print(f'\\r{idx_batch} / {len(dataloader)} train loss {loss.item()}', end='')\n",
    "        writer.add_scalar(f\"Train/loss\", loss.item(), writer.train_step)\n",
    "        writer.add_scalar(\"Train/grad_norm\", grad_norm, writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_enc\", get_enc_lr(optimizer), writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_dec\", get_dec_lr(optimizer), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "        # make a gradient step\n",
    "        if (idx_batch + 1) % optimizer.accumulation_interval == 0 or (idx_batch + 1) == len(dataloader):\n",
    "            logging.debug('optimizer step')\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        logging.debug('scheduler step')\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"\\rTrain loss:\", loss_val / len(dataloader), f\"{100 * loss_val / target_val:.5f}%\")\n",
    "    # print(\"Train mean sent len:\", mean_sents / szs)\n",
    "\n",
    "    # save model, just in case\n",
    "    model.save('temp')\n",
    "    logging.debug('save model to temp')\n",
    "\n",
    "    return model, optimizer, scheduler, writer\n",
    "\n",
    "\n",
    "def evaluate_fun(\n",
    "        model,\n",
    "        dataloader,\n",
    "        criter,\n",
    "        device,\n",
    "        writer,\n",
    "        additional_features\n",
    "):\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "    loss_val = 0\n",
    "    target_val = 0\n",
    "    mean_sents = 0\n",
    "    szs = 0\n",
    "\n",
    "    for idx_batch, batch in enumerate(dataloader):\n",
    "        input_ids, attention_mask, token_type_ids, target_scores, input_features = \\\n",
    "            batch_to_device(batch, additional_features, device)\n",
    "        sizes = [dc.shape[0] for dc in target_scores]\n",
    "        mean_sents += sum(sizes)\n",
    "        szs += len(sizes)\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "\n",
    "        # evaluate pass\n",
    "        logging.debug('evaluate')\n",
    "        logging.debug(f'input_ids {input_ids.shape}')\n",
    "        logging.debug(f'attention_mask {attention_mask.shape}')\n",
    "        logging.debug(f'token_type_ids {token_type_ids.shape}')\n",
    "        if additional_features:\n",
    "            logging.debug(f'input_features {input_features.shape}')\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "            model_scores = model(input_ids, attention_mask, token_type_ids, input_features)\n",
    "        logging.debug(f'model_scores {model_scores.shape}')\n",
    "        logging.debug(f'target_scores {len(target_scores)}')\n",
    "\n",
    "        # loss\n",
    "        loss = criter(model_scores, target_scores, )\n",
    "        target_val += sum(target_scores) / len(target_scores)\n",
    "\n",
    "        # record a loss value\n",
    "        logging.debug(f'{idx_batch} / {len(dataloader)} val loss {loss.item()}')\n",
    "        print(f'\\r{idx_batch} / {len(dataloader)} val loss {loss.item()}', end='')\n",
    "        loss_val += loss.item()\n",
    "        writer.add_scalar(f\"Eval/loss\", loss.item(), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "    print(\"\\rValidate loss:\", loss_val / len(dataloader), f\"{100 * loss_val / target_val:.5f}%\")\n",
    "    # print(\"Validate mean sent len:\", mean_sents / szs)\n",
    "\n",
    "    # save model, just in case\n",
    "    model.save('validated_weights')\n",
    "    logging.debug('save model to validated_weights')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset and dataloader classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "OVERLAP = 5\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \"\"\" Custom Train Dataset for data with additional features.\n",
    "        First preprocess all the data and then give out the batches.\n",
    "        It implements overlapping between batches to keep context between train examples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len, additional_features):\n",
    "        self.df = dataframe\n",
    "        self.data = []\n",
    "        self.pmids = list(set(dataframe['pmid'].values))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "        self.additional_features = additional_features\n",
    "        # Create a list of test inputs for each pmid\n",
    "        for pmid in tqdm(self.pmids):\n",
    "            ex = self.df[self.df['pmid'] == pmid]\n",
    "            text = ex['sentence'].values\n",
    "            features = np.nan_to_num(\n",
    "                ex[['sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "                    'mean_r_fig', 'mean_r_tab',\n",
    "                    'min_r_fig', 'min_r_tab',\n",
    "                    'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "            ) if additional_features else None\n",
    "            # Preprocessing BERT cannot encode all the text,\n",
    "            # only limited number of sentences per single model run is supported.\n",
    "            total_sents = 0\n",
    "            while total_sents < len(text):\n",
    "                offset = max(0, total_sents - OVERLAP)\n",
    "                input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(\n",
    "                    text[offset:], self.article_len, self.tokenizer\n",
    "                )\n",
    "                if n_sents <= OVERLAP:\n",
    "                    total_sents += 1\n",
    "                    continue\n",
    "                total_sents = offset + n_sents\n",
    "                target_scores = ex['score'].values[offset: offset + n_sents] / 100\n",
    "                input_features = features[offset: offset + n_sents] if additional_features else None\n",
    "                logging.debug(f'Train dataset example {len(self.data)}\\n'\n",
    "                              f'input_ids {input_ids}\\n'\n",
    "                              f'attention_mask {attention_mask}\\n'\n",
    "                              f'token_type_ids {token_type_ids}\\n'\n",
    "                              f'target_scores {target_scores}\\n'\n",
    "                              f'features {input_features}')\n",
    "                if additional_features:\n",
    "                    self.data.append((input_ids, attention_mask, token_type_ids, target_scores, input_features))\n",
    "                else:\n",
    "                    self.data.append((input_ids, attention_mask, token_type_ids, target_scores))\n",
    "\n",
    "        logging.info(f'Train dataset size {len(self.data)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    \"\"\" Custom Valid/Test Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len, additional_features):\n",
    "        self.df = dataframe\n",
    "        self.pmids = list(set(dataframe['pmid'].values))\n",
    "        logging.info(f'Eval dataset size {len(self.pmids)}')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "        self.additional_features = additional_features\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pmid = self.pmids[idx]\n",
    "        ex = self.df[self.df['pmid'] == pmid]\n",
    "        paper = ex['sentence'].values\n",
    "        features = np.nan_to_num(\n",
    "            ex[['sent_id', 'sent_type', 'r_abs',\n",
    "                'num_refs', 'mean_r_fig', 'mean_r_tab',\n",
    "                'min_r_fig', 'min_r_tab',\n",
    "                'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "        ) if self.additional_features else None\n",
    "        input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(\n",
    "            paper, self.article_len, self.tokenizer\n",
    "        )\n",
    "\n",
    "        # form target\n",
    "        target_scores = ex['score'].values[:n_sents] / 100\n",
    "        input_features = features[:n_sents] if self.additional_features else None\n",
    "        logging.debug(f'Eval dataset example {idx}\\n'\n",
    "                      f'input_ids {input_ids}\\n'\n",
    "                      f'attention_mask {attention_mask}\\n'\n",
    "                      f'token_type_ids {token_type_ids}\\n'\n",
    "                      f'target_scores {target_scores}\\n'\n",
    "                      f'features {input_features}')\n",
    "        if self.additional_features:\n",
    "            return input_ids, attention_mask, token_type_ids, target_scores, input_features\n",
    "        else:\n",
    "            return input_ids, attention_mask, token_type_ids, target_scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pmids)\n",
    "\n",
    "\n",
    "def create_collate_fn(additional_features):\n",
    "    \"\"\"Create Function to pull batch for train / eval.\"\"\"\n",
    "\n",
    "    def _collate_fn(batch_data):\n",
    "        \"\"\"\n",
    "        :param batch_data: list of `TrainDataset` or `EvalDataset` Examples\n",
    "        :return: one batch of data\n",
    "        \"\"\"\n",
    "        data = list(zip(*batch_data))\n",
    "        result = [\n",
    "            torch.tensor(data[0], dtype=torch.long),\n",
    "            torch.tensor(data[1], dtype=torch.long),\n",
    "            torch.tensor(data[2], dtype=torch.long),\n",
    "            [torch.tensor(e, dtype=torch.float) for e in data[3]]\n",
    "        ]\n",
    "        if additional_features:\n",
    "            result.append([torch.tensor(e, dtype=torch.float) for e in data[4]])\n",
    "        return result\n",
    "\n",
    "    return _collate_fn\n",
    "\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it\n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch\n",
    "# size of 16 or 32.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def get_dataloaders(train, val, batch_size, article_len, tokenizer, additional_features):\n",
    "    logging.info('Creating train dataset...')\n",
    "    train_ds = TrainDataset(train, tokenizer, article_len, additional_features)\n",
    "\n",
    "    logging.info('Applying loader functions to train...')\n",
    "    train_dl = DataLoader(\n",
    "        dataset=train_ds, batch_size=batch_size, shuffle=False,\n",
    "        pin_memory=True, collate_fn=create_collate_fn(additional_features), num_workers=cfg.num_workers\n",
    "    )\n",
    "\n",
    "    logging.info('Creating val dataset...')\n",
    "    val_ds = EvalDataset(val, tokenizer, article_len, additional_features)\n",
    "\n",
    "    logging.info('Applying loader functions to val...')\n",
    "    val_dl = DataLoader(\n",
    "        dataset=val_ds, batch_size=batch_size, shuffle=False,\n",
    "        pin_memory=True, collate_fn=create_collate_fn(additional_features), num_workers=cfg.num_workers\n",
    "    )\n",
    "\n",
    "    return train_dl, val_dl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom scheduler used in training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler, ExponentialLR\n",
    "\n",
    "\n",
    "class CustomScheduler(_LRScheduler):\n",
    "    timestep: int = 0\n",
    "\n",
    "    def __init__(self, optimizer, gamma, warmup=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.after_warmup = ExponentialLR(optimizer, gamma=gamma)\n",
    "        self.initial_lrs = [p_group['lr'] for p_group in self.optimizer.param_groups]\n",
    "        self.warmup = 0 if warmup is None else warmup\n",
    "        super(CustomScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.timestep * group_init_lr / self.warmup for group_init_lr in\n",
    "                self.initial_lrs] if self.timestep < self.warmup else self.after_warmup.get_lr()\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if self.timestep < self.warmup:\n",
    "            self.timestep += 1\n",
    "            super(CustomScheduler, self).step(epoch)\n",
    "        else:\n",
    "            self.after_warmup.step(epoch)\n",
    "\n",
    "\n",
    "class NoamScheduler(_LRScheduler):\n",
    "    \"\"\"\n",
    "    Noam optimizer has a warm-up period and then an exponentially decaying learning.\n",
    "    This is the PyTorch implementation of optimizer introduced in the paper \"Attention is all you need\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup):\n",
    "        assert warmup > 0\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lrs = [p_group['lr'] for p_group in self.optimizer.param_groups]\n",
    "        self.warmup = warmup\n",
    "        self.timestep = 0\n",
    "        super(NoamScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        noam_lr = self.get_noam_lr()\n",
    "        return [group_init_lr * noam_lr for group_init_lr in self.initial_lrs]\n",
    "\n",
    "    def get_noam_lr(self):\n",
    "        return min(self.timestep ** -0.5, self.timestep * self.warmup ** -1.5)\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        self.timestep += 1\n",
    "        super(NoamScheduler, self).step(epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare and configure training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import MSELoss\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENCODER_LEARNING_RATE = 0.0001\n",
    "DECODER_LEARNING_RATE = 0.001\n",
    "\n",
    "WARMUP = 5\n",
    "WEIGHT_DECAY = 0.01\n",
    "CLIP_VALUE = 1.0\n",
    "ACCUMULATION_INTERVAL = 1\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4.\n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "EPOCHS_NUMBER = 10\n",
    "\n",
    "\n",
    "def prepare_learning_tools(\n",
    "        model,\n",
    "        enc_lr=ENCODER_LEARNING_RATE,\n",
    "        dec_lr=DECODER_LEARNING_RATE,\n",
    "        warmup=WARMUP,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        clip_value=CLIP_VALUE,\n",
    "        accumulation_interval=ACCUMULATION_INTERVAL\n",
    "):\n",
    "    # TODO fix for Roberta model\n",
    "    enc_parameters = [\n",
    "        param for name, param in model.named_parameters()\n",
    "        if param.requires_grad and name.startswith('bert.')\n",
    "    ]\n",
    "    dec_parameters = [\n",
    "        param for name, param in model.named_parameters() if param.requires_grad\n",
    "    ]\n",
    "    optimizer = AdamW([\n",
    "        dict(params=enc_parameters, lr=enc_lr),\n",
    "        dict(params=dec_parameters, lr=dec_lr),\n",
    "    ], weight_decay=weight_decay)\n",
    "    optimizer.clip_value = clip_value\n",
    "    optimizer.accumulation_interval = accumulation_interval\n",
    "\n",
    "    scheduler = NoamScheduler(optimizer, warmup=warmup)\n",
    "    criter = MSELoss()\n",
    "\n",
    "    return optimizer, scheduler, criter\n",
    "\n",
    "\n",
    "def load_or_train_model(model, device, additional_features):\n",
    "    model_name = f'learn_simple_berta_{additional_features}.pth'.lower()\n",
    "    MODEL_PATH = f'{os.path.expanduser(cfg.weights_path)}/{model_name}'\n",
    "    # ! rm {MODEL_PATH}\n",
    "\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        logging.info(f'Loading model {MODEL_PATH}')\n",
    "        model.load(model_name)\n",
    "        model, device = setup_cuda_device(model)\n",
    "        return model\n",
    "    else:\n",
    "        logging.info('Create dataloaders...')\n",
    "        train_loader, valid_loader = get_dataloaders(\n",
    "            train, val, BATCH_SIZE, ARTICLE_LENGTH, model.tokenizer, additional_features\n",
    "        )\n",
    "\n",
    "        writer = SummaryWriter(log_dir=os.path.expanduser(cfg.log_path))\n",
    "        writer.train_step, writer.eval_step = 0, 0\n",
    "\n",
    "        optimizer, scheduler, criter = prepare_learning_tools(model)\n",
    "\n",
    "        logging.info(f\"Start training {EPOCHS_NUMBER} epochs...\")\n",
    "        for epoch in tqdm(range(1, EPOCHS_NUMBER + 1)):\n",
    "            print(f'Epoch {epoch}')\n",
    "            model, optimizer, scheduler, writer = train_fun(\n",
    "                model, train_loader, optimizer, scheduler,\n",
    "                criter, device, writer, additional_features\n",
    "            )\n",
    "            model = evaluate_fun(\n",
    "                model, valid_loader, criter, device, writer, additional_features\n",
    "            )\n",
    "        logging.info(f\"Done training {EPOCHS_NUMBER} epochs...\")\n",
    "        logging.info(f'Save trained model to {model_name}')\n",
    "        model.save(model_name)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create and train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ARTICLE_LENGTH = 512\n",
    "\n",
    "model = create_model(\"bert\", \"froze_all\", ARTICLE_LENGTH, additional_features=ADDITIONAL_FEATURES)\n",
    "model, device = setup_cuda_device(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_or_train_model(model, device, additional_features=ADDITIONAL_FEATURES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example of model predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Prepare data for model')\n",
    "ex = val[val['pmid'] == val['pmid'].values[0]]\n",
    "print('ex', len(ex))\n",
    "text = ex['sentence'].values\n",
    "\n",
    "input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(\n",
    "    text, ARTICLE_LENGTH, model.tokenizer\n",
    ")\n",
    "print('n_sents', n_sents)\n",
    "res_sents = text[:n_sents]\n",
    "scores = ex['score'].values[:n_sents]\n",
    "\n",
    "input_ids = torch.tensor([input_ids]).to(device)\n",
    "attention_mask = torch.tensor([attention_mask]).to(device)\n",
    "token_type_ids = torch.tensor([token_type_ids]).to(device)\n",
    "\n",
    "if ADDITIONAL_FEATURES:\n",
    "    features = np.nan_to_num(\n",
    "        ex[['sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "            'mean_r_fig', 'mean_r_tab',\n",
    "            'min_r_fig', 'min_r_tab',\n",
    "            'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "    )\n",
    "    features = features[:n_sents]\n",
    "    features = [torch.tensor(e, dtype=torch.float) for e in features]\n",
    "    features = torch.stack(features).to(device)\n",
    "else:\n",
    "    features = None\n",
    "\n",
    "print('Apply model')\n",
    "model_scores = model(input_ids, attention_mask, token_type_ids, features)\n",
    "\n",
    "to_show_df = pd.DataFrame(\n",
    "    dict(sentence=res_sents, ideal_score=scores / 100, res_score=model_scores.cpu().detach().numpy())\n",
    ")\n",
    "display(to_show_df.head())\n",
    "print(((to_show_df['ideal_score'].values - to_show_df['res_score'].values) ** 2).mean() ** 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate model performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 'model' not in globals():\n",
    "    model = create_model(\"bert\", \"froze_all\", ARTICLE_LENGTH, additional_features=ADDITIONAL_FEATURES)\n",
    "    model, device = setup_cuda_device(model)\n",
    "    model = load_or_train_model(model, device, additional_features=ADDITIONAL_FEATURES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Prepare refs_and_scores dataset')\n",
    "REF_SCORES_PATH = os.path.expanduser(f\"{cfg.base_path}/refs_and_scores.csv\")\n",
    "! rm {REF_SCORES_PATH}\n",
    "\n",
    "if os.path.exists(REF_SCORES_PATH):\n",
    "    final_ref_show_df = pd.read_csv(REF_SCORES_PATH)\n",
    "else:\n",
    "    to_show_ref = pd.merge(train_df, ref_sents_df[['ref_pmid', 'sentence']],\n",
    "                           left_on=['pmid'], right_on=['ref_pmid'])\n",
    "    to_show_ref = to_show_ref.rename(columns=dict(sentence_x='sentence', sentence_y='ref_sentence'))\n",
    "    to_show_ref = to_show_ref[['pmid', 'sentence', 'ref_sentence', 'score']]\n",
    "    final_ref_show_dic = dict(pmid=[], sentence=[], ref_sentence=[], score=[])\n",
    "    ite = [(pmid, sent) for pmid, sent in to_show_ref[['pmid', 'sentence']].values]\n",
    "\n",
    "    for pmid, sent in tqdm(set(ite)):\n",
    "        refs_df = to_show_ref[(to_show_ref['pmid'] == pmid) & (to_show_ref['sentence'] == sent)]\n",
    "        final_ref_show_dic['pmid'].append(pmid)\n",
    "        final_ref_show_dic['sentence'].append(sent)\n",
    "        final_ref_show_dic['ref_sentence'].append(\" \".join(refs_df['ref_sentence'].values))\n",
    "        final_ref_show_dic['score'].append(refs_df['score'].values[0])\n",
    "    final_ref_show_df = pd.DataFrame(final_ref_show_dic)\n",
    "    final_ref_show_df.to_csv(REF_SCORES_PATH, index=False)\n",
    "\n",
    "display(final_ref_show_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Prepare dataset to estimate performance')\n",
    "to_test = final_ref_show_df[final_ref_show_df['pmid'].isin(set(val['pmid'].values))]\n",
    "if ADDITIONAL_FEATURES:\n",
    "    to_test = pd.merge(to_test, train_df[['pmid', 'sentence',\n",
    "                                          'sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "                                          'mean_r_fig', 'mean_r_tab',\n",
    "                                          'min_r_fig', 'min_r_tab',\n",
    "                                          'max_r_fig', 'max_r_tab']],\n",
    "                       left_on=['pmid', 'sentence'], right_on=['pmid', 'sentence'])\n",
    "display(to_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Using model for predictions')\n",
    "res = dict(pmid=[], sentence=[], ref_sentences=[], score=[], res_score=[])\n",
    "\n",
    "for pmid in tqdm(set(to_test['pmid'].values)):\n",
    "    ex = to_test[to_test['pmid'] == pmid]\n",
    "    text = ex['sentence'].values\n",
    "    input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(\n",
    "        text, ARTICLE_LENGTH, model.tokenizer\n",
    "    )\n",
    "    res_sents = text[:n_sents]\n",
    "    scores = ex['score'].values[:n_sents] / 100\n",
    "    input_ids = torch.tensor([input_ids]).to(device)\n",
    "    attention_mask = torch.tensor([attention_mask]).to(device)\n",
    "    token_type_ids = torch.tensor([token_type_ids]).to(device)\n",
    "    if ADDITIONAL_FEATURES:\n",
    "        features = np.nan_to_num(\n",
    "            ex[['sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "                'mean_r_fig', 'mean_r_tab',\n",
    "                'min_r_fig', 'min_r_tab',\n",
    "                'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "        )\n",
    "        features = features[:n_sents]\n",
    "        input_features = [torch.tensor(e, dtype=torch.float) for e in features]\n",
    "        input_features = torch.stack(input_features).to(device)\n",
    "        model_scores = model(input_ids, attention_mask, token_type_ids, input_features)\n",
    "    else:\n",
    "        model_scores = model(input_ids, attention_mask, token_type_ids)\n",
    "    for sent, sc, res_sc in zip(res_sents, scores, model_scores.cpu().detach().numpy()):\n",
    "        res['pmid'].append(pmid)\n",
    "        res['sentence'].append(sent)\n",
    "        res['ref_sentences'].append(ex['ref_sentence'].values[0])\n",
    "        res['score'].append(sc)\n",
    "        res['res_score'].append(res_sc)\n",
    "\n",
    "res_df = pd.DataFrame(res)\n",
    "display(res_df.head())\n",
    "res_df.to_csv(f\"{cfg.base_path}/saved_example_refs.csv\")\n",
    "\n",
    "print('MSE score', ((res_df['score'].values - res_df['res_score'].values) ** 2).mean() ** 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quality analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 'model' not in globals():\n",
    "    model = create_model(\"bert\", \"froze_all\", ARTICLE_LENGTH, additional_features=ADDITIONAL_FEATURES)\n",
    "    model, device = setup_cuda_device(model)\n",
    "    model = load_or_train_model(model, device, additional_features=ADDITIONAL_FEATURES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Searching for review papers')\n",
    "inter = set(sentences_df['pmid'].values) & set(ref_sents_df['ref_pmid'].values)\n",
    "review_papers = list(set(ref_sents_df[ref_sents_df['ref_pmid'].isin(inter)]['pmid'].values))\n",
    "print('Review papers', len(review_papers))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_stat = dict(rev_pmid=[], sent_num=[], rouge=[], true_rouge=[], diff_papers=[])\n",
    "\n",
    "for rev_id in tqdm(review_papers):\n",
    "    paper_ref = sentences_df[sentences_df['pmid'] == rev_id]['sentence'].values\n",
    "    papers_to_check = list(set(ref_sents_df[ref_sents_df['pmid'] == rev_id]['ref_pmid'].values))\n",
    "    result = {'pmid': [], 'sentence': [], 'score': []}\n",
    "    for paper_id in papers_to_check:\n",
    "        ex = test[test['pmid'] == paper_id]\n",
    "        text = ex['sentence'].values\n",
    "\n",
    "        features = np.nan_to_num(\n",
    "            ex[['sent_id', 'sent_type', 'r_abs',\n",
    "                'num_refs', 'mean_r_fig', 'mean_r_tab',\n",
    "                'min_r_fig', 'min_r_tab',\n",
    "                'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "        ) if ADDITIONAL_FEATURES else None\n",
    "        total_sents = 0\n",
    "        while total_sents < len(text):\n",
    "            magic = max(0, total_sents - 5)\n",
    "            input_ids, attention_mask, token_type_ids, n_sents = preprocess_paper_bert(\n",
    "                text[magic:], ARTICLE_LENGTH, model.tokenizer\n",
    "            )\n",
    "            if n_sents <= 5:\n",
    "                total_sents += 1\n",
    "                continue\n",
    "            old_total = total_sents\n",
    "            total_sents = magic + n_sents\n",
    "            input_ids = torch.tensor([input_ids]).to(device)\n",
    "            attention_mask = torch.tensor([attention_mask]).to(device)\n",
    "            token_type_ids = torch.tensor([token_type_ids]).to(device)\n",
    "            if ADDITIONAL_FEATURES:\n",
    "                input_features = [torch.tensor(e, dtype=torch.float) for e in features[magic:total_sents]]\n",
    "                input_features = torch.stack(input_features).to(device)\n",
    "                model_scores = model(input_ids, attention_mask, token_type_ids, input_features)\n",
    "            else:\n",
    "                model_scores = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            result['pmid'].extend([paper_id] * (total_sents - old_total))\n",
    "            result['sentence'].extend(list(text[old_total:total_sents]))\n",
    "            result['score'].extend(list(model_scores.cpu().detach().numpy())[old_total - magic:])\n",
    "\n",
    "    res_df = pd.DataFrame(result)\n",
    "    sorted_arr = sorted(list(res_df['score'].values))\n",
    "    for i in range(5, 103, 5):\n",
    "        if len(sorted_arr) < i:\n",
    "            break\n",
    "        threshold = sorted_arr[-i]\n",
    "        final_text = res_df[res_df['score'] >= threshold][['pmid', 'sentence']]\n",
    "        mean_score = 0\n",
    "        num = 0\n",
    "        for sent in final_text['sentence'].values:\n",
    "            for ref_sent in paper_ref:\n",
    "                try:\n",
    "                    mean_score += get_rouge(sent, ref_sent)\n",
    "                    num += 1\n",
    "                except Exception:\n",
    "                    continue\n",
    "        mean_score /= num\n",
    "        real_score = get_rouge(\" \".join(final_text['sentence'].values), \" \".join(paper_ref))\n",
    "        test_stat['rev_pmid'].append(rev_id)\n",
    "        test_stat['sent_num'].append(i)\n",
    "        print(len(\" \".join(final_text['sentence'].values)), len(\" \".join(paper_ref)))\n",
    "\n",
    "        test_stat['rouge'].append(mean_score)\n",
    "        test_stat['true_rouge'].append(real_score)\n",
    "        test_stat['diff_papers'].append(len(set(final_text['pmid'])))\n",
    "\n",
    "test_stat_df = pd.DataFrame(test_stat)\n",
    "test_stat_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(*[len(arr) for key, arr in test_stat.items()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_stat_df.to_csv(f\"{cfg.base_path}/simple_right_test_on_review_{ADDITIONAL_FEATURES}.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rouge_means = []\n",
    "rouge_err = []\n",
    "papers_means = []\n",
    "papers_err = []\n",
    "\n",
    "for i in range(5, 103, 5):\n",
    "    tmp = test_stat_df.groupby(['sent_num']).get_group(i)\n",
    "    rouge_means.append(tmp['rouge'].mean())\n",
    "    rouge_err.append(tmp['rouge'].std())\n",
    "    papers_means.append(tmp['diff_papers'].mean())\n",
    "    papers_err.append(tmp['diff_papers'].std())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.errorbar(list(range(5, 103, 5)), rouge_means, yerr=rouge_err, fmt='-o')\n",
    "plt.title('Mean rouge value')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.errorbar(list(range(5, 103, 5)), papers_means, yerr=papers_err, fmt='-o')\n",
    "plt.title('Mean number of papers')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare models with and without features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(f\"{cfg.base_path}/simple_right_test_on_review_{False}.csv\", index=False)\n",
    "df1 = df1.assign(model=['BERTSUM'] * len(df1))\n",
    "df2 = pd.read_csv(f\"{cfg.base_path}/simple_right_test_on_review_{True}.csv\", index=False)\n",
    "df2 = df1.assign(model=['BERTSUM with features'] * len(df1))\n",
    "draw_df = pd.concat([df1, df2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.catplot(x=\"sent_num\", y=\"rouge\", kind=\"box\", hue='model', aspect=1.7, color='lightblue',\n",
    "            data=draw_df).set_axis_labels(\"Number of sentences\", \"ROUGE, %\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}