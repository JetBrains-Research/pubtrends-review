{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training&Evaluation of a developed algorithm\n",
    "\n",
    "This notebook contains source code for several possible architectures to train & evaluate them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import review.config as cfg\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logging.info('Check if CUDA is available')\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "def setup_cuda_device(model):\n",
    "    logging.info('Setup single-device settings...')\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    return model, device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logging.info('Fix seed')\n",
    "seed = cfg.seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading data and preparation of dataset\n",
    "\n",
    "`load_data` loads all the needed datafiles for building train dataset.\\\n",
    "The several next steps are only should be done if no train/test/val datasets are saved."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def load_data(additional_features):\n",
    "    dataset_root = os.path.expanduser(cfg.dataset_path)\n",
    "    logging.info(f'Loading references dataset from {dataset_root}')\n",
    "\n",
    "    logging.info('Loading citations_df')\n",
    "    citations_df = pd.read_csv(os.path.join(dataset_root, \"citations.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(citations_df)))\n",
    "\n",
    "    logging.info('Loading sentences_df')\n",
    "    sentences_df = pd.read_csv(os.path.join(dataset_root, \"sentences.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(sentences_df)))\n",
    "\n",
    "    logging.info('Loading review_files_df')\n",
    "    review_files_df = pd.read_csv(os.path.join(dataset_root, \"review_files.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(review_files_df)))\n",
    "\n",
    "    logging.info('Loading reverse_ref_df')\n",
    "    reverse_ref_df = pd.read_csv(os.path.join(dataset_root, \"reverse_ref.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(reverse_ref_df)))\n",
    "\n",
    "    if not additional_features:\n",
    "        return citations_df, sentences_df, review_files_df, reverse_ref_df\n",
    "\n",
    "    logging.info('Loading abstracts_df')\n",
    "    abstracts_df = pd.read_csv(os.path.join(dataset_root, \"abstracts.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(abstracts_df)))\n",
    "\n",
    "    logging.info('Loading figures_df')\n",
    "    figures_df = pd.read_csv(os.path.join(dataset_root, \"figures.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(figures_df)))\n",
    "\n",
    "    logging.info('Loading tables_df')\n",
    "    tables_df = pd.read_csv(os.path.join(dataset_root, \"tables.csv\"), sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(tables_df)))\n",
    "\n",
    "    return citations_df, sentences_df, review_files_df, reverse_ref_df, abstracts_df, figures_df, tables_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# citations_df, sentences_df, review_files_df, reverse_ref_df = load_data(additional_features=False)\n",
    "\n",
    "# Uncomment to load additional features\n",
    "citations_df, sentences_df, review_files_df, reverse_ref_df, abstracts_df, figures_df, tables_df = load_data(\n",
    "    additional_features=True)\n",
    "\n",
    "logging.info('Done loading references dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "res = Counter(list(sentences_df['pmid'].values))\n",
    "plt.hist(res.values(), bins=range(-1, 400))\n",
    "plt.title('Length of papers')\n",
    "plt.show()\n",
    "del res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "REF_SENTS_DF_PATH = f\"{os.path.expanduser(cfg.base_path)}/ref_sents.csv\"\n",
    "! rm {REF_SENTS_DF_PATH}\n",
    "\n",
    "if os.path.exists(REF_SENTS_DF_PATH):\n",
    "    ref_sents_df = pd.read_csv(REF_SENTS_DF_PATH, sep='\\t')\n",
    "else:\n",
    "    logging.info('Creating reference sentences dataset')\n",
    "    ref_sents_df = pd.merge(citations_df, reverse_ref_df, left_on=['pmid', 'ref_id'], right_on=['pmid', 'ref_id'])\n",
    "    ref_sents_df = pd.merge(ref_sents_df, sentences_df, left_on=['pmid', 'sent_type', 'sent_id'],\n",
    "                            right_on=['pmid', 'type', 'sent_id'])\n",
    "    ref_sents_df = ref_sents_df[ref_sents_df['pmid'].isin(review_files_df['pmid'].values)]\n",
    "    ref_sents_df = ref_sents_df.drop_duplicates()\n",
    "    logging.info(f'Len of unique ref_sents {len(set(ref_sents_df[\"ref_pmid\"]))}')\n",
    "    ref_sents_df = ref_sents_df[['pmid', 'ref_id', 'pub_type', 'ref_pmid', 'sent_type', 'sent_id', 'sentence']]\n",
    "    ref_sents_df.to_csv(REF_SENTS_DF_PATH, sep='\\t', index=False)\n",
    "\n",
    "logging.info('Cleanup memory')\n",
    "del citations_df\n",
    "del review_files_df\n",
    "\n",
    "display(ref_sents_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rouge\n",
    "`get_rouge` function allows to compute similarity between two sentences.\n",
    "`rouge-l` is another possible option."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "ROUGE_METER = Rouge()\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "def get_rouge(sent1, sent2):\n",
    "    if sent1 is None or sent2 is None:\n",
    "        return None\n",
    "    sent_1 = TOKENIZER.tokenize(sent1)\n",
    "    if len(sent_1) == 0:\n",
    "        return None\n",
    "    sent_1 = \" \".join(list(filter(lambda x: x.isalpha() or x in '.!,?', sent_1)))\n",
    "    sent_2 = TOKENIZER.tokenize(sent2)\n",
    "    if len(sent_2) == 0:\n",
    "        return None\n",
    "    sent_2 = \" \".join(list(filter(lambda x: x.isalpha() or x in '.!,?', sent_2)))\n",
    "    if len(sent_1) == 0 or len(sent_2) == 0:\n",
    "        return None\n",
    "    rouges = ROUGE_METER.get_scores(sent_1, sent_2)[0]\n",
    "    rouges = [rouges[f'rouge-{x}'][\"f\"] for x in ('1', '2')]  # , 'l')]\n",
    "    return np.mean(rouges) * 100\n",
    "\n",
    "\n",
    "def mean_rouge(sent, text):\n",
    "    if len(text) == 0:\n",
    "        return None\n",
    "    try:\n",
    "        return sum(get_rouge(sent, ref_sent) for ref_sent in text) / len(text)\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception at mean_rouge {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def min_rouge(sent, text):\n",
    "    try:\n",
    "        score = 100000000\n",
    "        for ref_sent in text:\n",
    "            score = min(get_rouge(sent, ref_sent), score)\n",
    "        if score == 100000000:\n",
    "            return None\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception at min_rouge {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def max_rouge(sent, text):\n",
    "    try:\n",
    "        score = -100000\n",
    "        for ref_sent in text:\n",
    "            score = max(get_rouge(sent, ref_sent), score)\n",
    "        if score == -100000:\n",
    "            return None\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        logging.error(f'Exception at max_rouge {e}')\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_rouge(\"I am scout. True!\", \"No you are not a scout.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build train / test/ validate datasets\n",
    "\n",
    "To create a dataset with features use `preprocess_paper_with_features`. Otherwise, use `preprocess_paper`.\n",
    "\n",
    "In case datasets are not yet created and `ref_sents_df` is also not yet created, let's create `ref_sents_df`.\\\n",
    "For each paper pmid there is a list of sentences from review papers in which the paper with this `pmid` is cited."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "REPLACE_SYMBOLS = {\n",
    "    '—': '-',\n",
    "    '–': '-',\n",
    "    '―': '-',\n",
    "    '…': '...',\n",
    "    '´´': \"´\",\n",
    "    '´´´': \"´´\",\n",
    "    \"''\": \"'\",\n",
    "    \"'''\": \"'\",\n",
    "    \"``\": \"`\",\n",
    "    \"```\": \"`\",\n",
    "    \":\": \" : \",\n",
    "}\n",
    "\n",
    "\n",
    "def parse_sents(data):\n",
    "    sents = sum([sent_tokenize(text) for text in data], [])\n",
    "    sents = list(filter(lambda x: len(x) > 3, sents))\n",
    "    return sents\n",
    "\n",
    "\n",
    "def sent_standardize(sent):\n",
    "    sent = unidecode(sent)\n",
    "    sent = re.sub(r\"\\[(xref_\\w*_\\w\\d*]*)(, xref_\\w*_\\w\\d*)*\\]\", \" \", sent)  # delete [xref,...]\n",
    "    sent = re.sub(r\"\\( (xref_\\w*_\\w\\d*)(; xref_\\w*_\\w\\d*)* \\)\", \" \", sent)  # delete (xref; ...)\n",
    "    sent = re.sub(r\"\\[xref_\\w*_\\w\\d*\\]\", \" \", sent)  # delete [xref]\n",
    "    sent = re.sub(r\"xref_\\w*_\\w\\d*\", \" \", sent)  # delete [[xref]]\n",
    "    for k, v in REPLACE_SYMBOLS.items():\n",
    "        sent = sent.replace(k, v)\n",
    "    return sent.strip()\n",
    "\n",
    "\n",
    "def standardize(text):\n",
    "    return [x for x in (sent_standardize(sent) for sent in text) if len(x) > 3]\n",
    "\n",
    "\n",
    "def preprocess_paper(paper_id, sentences_df, ref_sents_df, min_paper_sents=50, max_paper_sents=100):\n",
    "    paper = sentences_df[sentences_df['pmid'] == paper_id]['sentence']\n",
    "    paper = standardize(paper)\n",
    "\n",
    "    ref_sents = ref_sents_df[ref_sents_df['ref_pmid'] == paper_id]['sentence']\n",
    "    ref_sents = standardize(ref_sents)\n",
    "\n",
    "    if len(paper) < min_paper_sents:\n",
    "        return None\n",
    "\n",
    "    if len(paper) > max_paper_sents:\n",
    "        paper = list(paper[:min_paper_sents]) + list(paper[-min_paper_sents:])\n",
    "\n",
    "    preprocessed_score = [\n",
    "        sum(get_rouge(sent, ref_sent) for ref_sent in ref_sents) / len(ref_sents) for sent in paper\n",
    "    ]\n",
    "    return paper, preprocessed_score\n",
    "\n",
    "\n",
    "def preprocess_paper_with_features(paper_id, sentences_df, ref_sents_df,\n",
    "                                   abstracts_df, figures_df, reverse_ref_df, tables_df,\n",
    "                                   min_paper_sents=50, max_paper_sents=100):\n",
    "    preprocessed_score = []\n",
    "    features = []\n",
    "\n",
    "    papers = sentences_df[sentences_df['pmid'] == paper_id]['sentence']\n",
    "    papers = standardize(papers)\n",
    "\n",
    "    sent_ids = sentences_df[sentences_df['pmid'] == paper_id]['sent_id']\n",
    "    sent_types = sentences_df[sentences_df['pmid'] == paper_id]['type']\n",
    "\n",
    "    ref_sents = ref_sents_df[ref_sents_df['ref_pmid'] == paper_id]['sentence']\n",
    "    ref_sents = standardize(ref_sents)\n",
    "\n",
    "    fig_captions = figures_df[figures_df['pmid'] == paper_id]['caption']\n",
    "    fig_captions = standardize(fig_captions)\n",
    "\n",
    "    tab_captions = tables_df[tables_df['pmid'] == paper_id]['caption']\n",
    "    tab_captions = standardize(tab_captions)\n",
    "\n",
    "    abstract = abstracts_df[abstracts_df['pmid'] == paper_id]['abstract']\n",
    "    if len(abstract) != 0:\n",
    "        abstract = standardize(abstract)\n",
    "\n",
    "    tmp_df = reverse_ref_df[reverse_ref_df['pmid'] == paper_id]\n",
    "\n",
    "    if len(papers) < min_paper_sents:\n",
    "        return None\n",
    "\n",
    "    if len(papers) > max_paper_sents:\n",
    "        papers = list(papers[:min_paper_sents]) + list(papers[-min_paper_sents:])\n",
    "        sent_ids = list(sent_ids[:min_paper_sents]) + list(sent_ids[-min_paper_sents:])\n",
    "        sent_types = list(sent_types[:min_paper_sents]) + list(sent_types[-min_paper_sents:])\n",
    "\n",
    "    for sent, sent_type, sent_id in zip(papers, sent_types, sent_ids):\n",
    "        score = mean_rouge(sent, ref_sents)\n",
    "        if score is None:\n",
    "            return None\n",
    "\n",
    "        r_abs = get_rouge(sent, abstract[0])\n",
    "        num_refs = len(tmp_df[(tmp_df['sent_type'] == sent_type) & (tmp_df['sent_id'] == sent_id)])\n",
    "        preprocessed_score.append(score)\n",
    "        features.append((sent_id, int(sent_type == \"general\"), r_abs, num_refs,\n",
    "                         mean_rouge(sent, fig_captions), mean_rouge(sent, tab_captions),\n",
    "                         min_rouge(sent, fig_captions), min_rouge(sent, tab_captions),\n",
    "                         max_rouge(sent, fig_captions), max_rouge(sent, tab_captions)))\n",
    "    return papers, preprocessed_score, features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_reference_sentences_dataset(sentences_df, ref_sents_df, additional_features=False):\n",
    "    res = {}\n",
    "    inter = set(sentences_df['pmid'].values) & set(ref_sents_df['ref_pmid'].values)\n",
    "    for pmid in tqdm(inter):\n",
    "        try:\n",
    "            if additional_features:\n",
    "                temp = preprocess_paper_with_features(\n",
    "                    pmid, sentences_df, ref_sents_df, abstracts_df, figures_df, reverse_ref_df, tables_df\n",
    "                )\n",
    "            else:\n",
    "                temp = preprocess_paper(pmid, sentences_df, ref_sents_df)\n",
    "        except Exception as e:\n",
    "            logging.warning(f'Error during processing {pmid} {e}', e)\n",
    "            continue\n",
    "        if temp is None:\n",
    "            logging.warning(f'temp is None for {pmid}')\n",
    "            continue\n",
    "        res[pmid] = temp\n",
    "        print(f\"\\r{pmid} {np.mean(res[pmid][1])}\", end=\"\")\n",
    "\n",
    "    logging.info(f'Successfully preprocessed {len(res)} of {len(inter)} papers')\n",
    "\n",
    "    logging.info(f'Creating train dataset')\n",
    "    feature_names = [\n",
    "        'sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "        'mean_r_fig', 'mean_r_tab', 'min_r_fig', 'min_r_tab', 'max_r_fig', 'max_r_tab'\n",
    "    ]\n",
    "    train_dic = dict(\n",
    "        pmid=[], sentence=[], score=[], sent_id=[], sent_type=[], r_abs=[], num_refs=[],\n",
    "        mean_r_fig=[], mean_r_tab=[], min_r_fig=[], min_r_tab=[], max_r_fig=[], max_r_tab=[]\n",
    "    )\n",
    "\n",
    "    for pmid, stat in tqdm(res.items()):\n",
    "        if len(stat) == 2:\n",
    "            for sent, score in zip(*stat):\n",
    "                train_dic['pmid'].append(pmid)\n",
    "                train_dic['sentence'].append(sent)\n",
    "                train_dic['score'].append(score)\n",
    "        else:\n",
    "            for sent, score, features in zip(*stat):\n",
    "                train_dic['pmid'].append(pmid)\n",
    "                train_dic['sentence'].append(sent)\n",
    "                train_dic['score'].append(score)\n",
    "                for name, val in zip(feature_names, features):\n",
    "                    train_dic[name].append(val)\n",
    "\n",
    "    train_df = pd.DataFrame({k: v for k, v in train_dic.items() if v})\n",
    "    logging.info(f'Full train dataset {len(train_df)}')\n",
    "    return train_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = f'{os.path.expanduser(cfg.base_path)}/dataset.csv'\n",
    "! rm {TRAIN_DATASET_PATH}\n",
    "\n",
    "if os.path.exists(TRAIN_DATASET_PATH):\n",
    "    train_df = pd.read_csv(TRAIN_DATASET_PATH)\n",
    "else:\n",
    "    train_df = process_reference_sentences_dataset(sentences_df, ref_sents_df, additional_features=True)\n",
    "    train_df.to_csv(TRAIN_DATASET_PATH, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(train_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing text for BERT\n",
    "A function to preprocess input text for `BERT` model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_paper_bert(text, max_len, tokenizer):\n",
    "    sents = [\n",
    "        [tokenizer.artBOS.tkn] + tokenizer.tokenize(sent) + [tokenizer.artEOS.tkn] for sent in text\n",
    "    ]\n",
    "    ids, segments, segment_signature = [], [], 0\n",
    "    n_sents = 0\n",
    "    for s in sents:\n",
    "        if len(ids) + len(s) <= max_len:\n",
    "            n_sents += 1\n",
    "            ids.extend(tokenizer.convert_tokens_to_ids(s))\n",
    "            segments.extend([segment_signature] * len(s))\n",
    "            segment_signature = (segment_signature + 1) % 2\n",
    "        else:\n",
    "            break\n",
    "    mask = [1] * len(ids)\n",
    "\n",
    "    pad_len = max(0, max_len - len(ids))\n",
    "    ids += [tokenizer.PAD.idx] * pad_len\n",
    "    mask += [0] * pad_len\n",
    "    segments += [segment_signature] * pad_len\n",
    "\n",
    "    return ids, mask, segments, n_sents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Splitting data into train/test/val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(list(set(train_df['pmid'].values)), test_size=0.2)\n",
    "test_ids, val_ids = train_test_split(test_ids, test_size=0.4)\n",
    "\n",
    "train = train_df[train_df['pmid'].isin(train_ids)]\n",
    "logging.info(f'Train {len(train)}')\n",
    "display(train.head(1))\n",
    "\n",
    "test = train_df[train_df['pmid'].isin(test_ids)]\n",
    "logging.info(f'Test {len(test)}')\n",
    "display(test.head(1))\n",
    "\n",
    "val = train_df[train_df['pmid'].isin(val_ids)]\n",
    "logging.info(f'Validate {len(val)}')\n",
    "display(val.head(1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model training functions\n",
    "\n",
    "`train_fun` -- training function for model without features. \\\n",
    "`train_fun_ft` -- training function for model with features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "\n",
    "\n",
    "def get_enc_lr(optimizer):\n",
    "    return optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "def get_dec_lr(optimizer):\n",
    "    return optimizer.param_groups[1]['lr']\n",
    "\n",
    "\n",
    "def backward_step(loss: torch.Tensor, optimizer: Optimizer, model: nn.Module, clip: float):\n",
    "    loss.backward()\n",
    "    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def train_fun(model, dataloader, optimizer, scheduler, criter, device, writer):\n",
    "    # draft, refine\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
    "    for idx_batch, batch in pbar:\n",
    "\n",
    "        input_ids, input_mask, input_segment, target_scores = [\n",
    "            (x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch\n",
    "        ]\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "\n",
    "        # forward pass\n",
    "        draft_probs = model(input_ids, input_mask, input_segment, )\n",
    "\n",
    "        try:\n",
    "            # loss\n",
    "            loss = criter(draft_probs, target_scores, )\n",
    "        except Exception:\n",
    "            print(idx_batch, draft_probs.shape, target_scores.shape, input_segment)\n",
    "            return\n",
    "\n",
    "        # backward\n",
    "        grad_norm = backward_step(loss, optimizer, model, optimizer.clip_value)\n",
    "        grad_norm = 0 if (math.isinf(grad_norm) or math.isnan(grad_norm)) else grad_norm\n",
    "\n",
    "        # record a loss value\n",
    "        # loss_val += loss.item() * len(input_ids)\n",
    "        pbar.set_description(f\"loss:{loss.item():.5f}\")\n",
    "        writer.add_scalar(f\"Train/loss\", loss.item(), writer.train_step)\n",
    "        writer.add_scalar(\"Train/grad_norm\", grad_norm, writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_enc\", get_enc_lr(optimizer), writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_dec\", get_dec_lr(optimizer), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "        # make a gradient step\n",
    "        if (idx_batch + 1) % optimizer.accumulation_interval == 0 or (idx_batch + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    # save model, just in case\n",
    "    model.save('temp')\n",
    "\n",
    "    return model, optimizer, scheduler, writer\n",
    "\n",
    "\n",
    "def train_fun_ft(model, dataloader, optimizer, scheduler, criter, device, writer):\n",
    "    # draft, refine\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
    "    for idx_batch, batch in pbar:\n",
    "\n",
    "        input_ids, input_mask, input_segment, target_scores, input_features = [\n",
    "            (x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch]\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "        input_features = torch.cat(input_features).to(device)\n",
    "        # forward pass\n",
    "        draft_probs = model(\n",
    "            input_ids, input_mask, input_segment, input_features,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # loss\n",
    "            loss = criter(draft_probs, target_scores, )\n",
    "        except Exception:\n",
    "            print(idx_batch, draft_probs.shape, target_scores.shape, input_segment)\n",
    "            return\n",
    "\n",
    "        # backward\n",
    "        grad_norm = backward_step(loss, optimizer, model, optimizer.clip_value)\n",
    "        grad_norm = 0 if (math.isinf(grad_norm) or math.isnan(grad_norm)) else grad_norm\n",
    "\n",
    "        # record a loss value\n",
    "        # loss_val += loss.item() * len(input_ids)\n",
    "        pbar.set_description(f\"loss:{loss.item():.5f}\")\n",
    "        writer.add_scalar(f\"Train/loss\", loss.item(), writer.train_step)\n",
    "        writer.add_scalar(\"Train/grad_norm\", grad_norm, writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_enc\", get_enc_lr(optimizer), writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_dec\", get_dec_lr(optimizer), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "        # make a gradient step\n",
    "        if (idx_batch + 1) % optimizer.accumulation_interval == 0 or (idx_batch + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    # save model, just in case\n",
    "    model.save('temp')\n",
    "\n",
    "    return model, optimizer, scheduler, writer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inspect pretrained models: BERT and Roberta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel, RobertaModel\n",
    "\n",
    "backbone = BertModel.from_pretrained(\n",
    "    \"bert-base-uncased\", output_hidden_states=False\n",
    ")\n",
    "print('BERT pretrained')\n",
    "print(f'Parameters {sum(p.numel() for p in backbone.parameters() if p.requires_grad)}')\n",
    "print(', '.join(n for n, p in backbone.named_parameters()))\n",
    "# print(backbone)\n",
    "\n",
    "backbone = RobertaModel.from_pretrained(\n",
    "    'roberta-base', output_hidden_states=False\n",
    ")\n",
    "print('ROBERTA pretrained')\n",
    "print(f'Parameters {sum(p.numel() for p in backbone.parameters() if p.requires_grad)}')\n",
    "print(', '.join(n for n, p in backbone.named_parameters()))\n",
    "# print(backbone)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main model classes\n",
    "\n",
    "The model in main pubtrends application is loaded using `load_model` function from `review.model` module.\n",
    "\n",
    "It has several options to set up:\n",
    "* with or without features (right now without features works better),\n",
    "* `BERT` or `roberta` as basis (no big difference),\n",
    "\n",
    "You can also choose `frozen_strategy`:\n",
    "* `froze_all` in case you don't want to improve bert layers but only the summarization layer,\n",
    "* `unfroze_last4` -- modifies bert weights and still training not very slow,\n",
    "* `unfroze_all` -- the training is slow, the results may better though"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from transformers import BertModel, RobertaModel\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "\n",
    "import review.config as cfg\n",
    "\n",
    "SpecToken = namedtuple('SpecToken', ['tkn', 'idx'])\n",
    "ConvertToken2Id = lambda tokenizer, tkn: tokenizer.convert_tokens_to_ids([tkn])[0]\n",
    "\n",
    "\n",
    "class Summarizer(nn.Module):\n",
    "    enc_output: torch.Tensor\n",
    "    dec_ids_mask: torch.Tensor\n",
    "    encdec_ids_mask: torch.Tensor\n",
    "\n",
    "    def __init__(self, model_type, article_len, additional_features=False, num_features=10):\n",
    "        super(Summarizer, self).__init__()\n",
    "\n",
    "        self.article_len = article_len\n",
    "\n",
    "        if model_type == 'bert':\n",
    "            self.backbone, self.tokenizer, BOS, EOS, PAD = self.initialize_bert()\n",
    "        elif model_type == 'roberta':\n",
    "            self.backbone, self.tokenizer, BOS, EOS, PAD = self.initialize_roberta()\n",
    "        else:\n",
    "            raise Exception(f\"Wrong model_type argument: {model_type}\")\n",
    "\n",
    "        if additional_features:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Linear(num_features, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 50)\n",
    "            )\n",
    "        else:\n",
    "            self.features = None\n",
    "\n",
    "        self.PAD = SpecToken(PAD, ConvertToken2Id(self.tokenizer, PAD))\n",
    "        self.artBOS = SpecToken(BOS, ConvertToken2Id(self.tokenizer, BOS))\n",
    "        self.artEOS = SpecToken(EOS, ConvertToken2Id(self.tokenizer, EOS))\n",
    "\n",
    "        # add special tokens tokenizer\n",
    "        self.tokenizer.add_special_tokens({'additional_special_tokens': [\"<sum>\", \"</sent>\", \"</sum>\"]})\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "        self.sumBOS = SpecToken(\"<sum>\", ConvertToken2Id(self.tokenizer, \"<sum>\"))\n",
    "        self.sumEOS = SpecToken(\"</sent>\", ConvertToken2Id(self.tokenizer, \"</sent>\"))\n",
    "        self.sumEOA = SpecToken(\"</sum>\", ConvertToken2Id(self.tokenizer, \"</sum>\"))\n",
    "        self.backbone.resize_token_embeddings(200 + self.vocab_size)\n",
    "\n",
    "        # tokenizer\n",
    "        self.tokenizer.PAD = self.PAD\n",
    "        self.tokenizer.artBOS = self.artBOS\n",
    "        self.tokenizer.artEOS = self.artEOS\n",
    "        self.tokenizer.sumBOS = self.sumBOS\n",
    "        self.tokenizer.sumEOS = self.sumEOS\n",
    "        self.tokenizer.sumEOA = self.sumEOA\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "\n",
    "        # initialize backbone emb pulling\n",
    "        def backbone_forward(input_ids, input_mask, input_segment, input_pos):\n",
    "            return self.backbone(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=input_mask,\n",
    "                token_type_ids=input_segment,\n",
    "                position_ids=input_pos,\n",
    "            )\n",
    "\n",
    "        self.encoder = lambda *args: backbone_forward(*args)[0]\n",
    "\n",
    "        # initialize decoder\n",
    "        if not additional_features:\n",
    "            self.decoder = Classifier(cfg.d_hidden)\n",
    "        else:\n",
    "            self.decoder = Classifier(cfg.d_hidden + 50)\n",
    "\n",
    "    def expand_posembs_ifneed(self):\n",
    "        print(self.backbone.config.max_position_embeddings, self.article_len)\n",
    "        if self.article_len > self.backbone.config.max_position_embeddings:\n",
    "            print(\"OK\")\n",
    "            old_maxlen = self.backbone.config.max_position_embeddings\n",
    "            old_w = self.backbone.embeddings.position_embeddings.weight\n",
    "            logging.info(f\"Backbone pos embeddings expanded from {old_maxlen} upto {self.article_len}\")\n",
    "            self.backbone.embeddings.position_embeddings = nn.Embedding(self.article_len,\n",
    "                                                                        self.backbone.config.hidden_size)\n",
    "            self.backbone.embeddings.position_embeddings.weight[:old_maxlen].data.copy_(old_w)\n",
    "            self.backbone.config.max_position_embeddings = self.article_len\n",
    "        print(self.backbone.config.max_position_embeddings)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_bert():\n",
    "        backbone = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", output_hidden_states=False\n",
    "        )\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "        BOS = \"[CLS]\"\n",
    "        EOS = \"[SEP]\"\n",
    "        PAD = \"[PAD]\"\n",
    "        return backbone, tokenizer, BOS, EOS, PAD\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_roberta():\n",
    "        backbone = RobertaModel.from_pretrained(\n",
    "            'roberta-base', output_hidden_states=False\n",
    "        )\n",
    "        # initialize token type emb, by default roberta doesn't have it\n",
    "        backbone.config.type_vocab_size = 2\n",
    "        backbone.embeddings.token_type_embeddings = nn.Embedding(2, backbone.config.hidden_size)\n",
    "        backbone.embeddings.token_type_embeddings.weight.data.normal_(\n",
    "            mean=0.0, std=backbone.config.initializer_range\n",
    "        )\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "        BOS = \"<s>\"\n",
    "        EOS = \"</s>\"\n",
    "        PAD = \"<pad>\"\n",
    "        return backbone, tokenizer, BOS, EOS, PAD\n",
    "\n",
    "    def save(self, save_filename):\n",
    "        \"\"\" Save model in filename\n",
    "\n",
    "        :param save_filename: str\n",
    "        \"\"\"\n",
    "        state = dict(\n",
    "            encoder_dict=self.backbone.state_dict(),\n",
    "            decoder_dict=self.decoder.state_dict()\n",
    "        )\n",
    "        if self.features:\n",
    "            state['features_dict'] = self.features.state_dict()\n",
    "        models_folder = os.path.expanduser(cfg.weights_path)\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.makedirs(models_folder)\n",
    "        torch.save(state, f\"{models_folder}/{save_filename}.pth\")\n",
    "\n",
    "    def load(self, load_filename):\n",
    "        path = f\"{os.path.expanduser(cfg.weights_path)}/{load_filename}.pth\"\n",
    "        state = torch.load(path, map_location=lambda storage, location: storage)\n",
    "        self.backbone.load_state_dict(state['encoder_dict'])\n",
    "        self.decoder.load_state_dict(state['decoder_dict'])\n",
    "        if self.features:\n",
    "            self.features.load_state_dict(state['features_dict'])\n",
    "\n",
    "    def froze_backbone(self, froze_strategy):\n",
    "        assert froze_strategy in ['froze_all', 'unfroze_last4',\n",
    "                                  'unfroze_all'], f\"incorrect froze_strategy argument: {froze_strategy}\"\n",
    "\n",
    "        if froze_strategy == 'froze_all':\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        elif froze_strategy == 'unfroze_last4':\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                param.requires_grad_(\n",
    "                    'encoder.layer.11' in name or\n",
    "                    'encoder.layer.10' in name or\n",
    "                    'encoder.layer.9' in name or\n",
    "                    'encoder.layer.8' in name\n",
    "                )\n",
    "\n",
    "        elif froze_strategy == 'unfroze_all':\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad_(True)\n",
    "\n",
    "    def unfroze_head(self):\n",
    "        for name, param in self.decoder.named_parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    def forward(self, input_ids, input_mask, input_segment, input_features=None):\n",
    "        \"\"\" Train for 1st stage of model\n",
    "        :param input_ids: torch.Size([batch_size, article_len])\n",
    "        :param input_mask: torch.Size([batch_size, article_len])\n",
    "        :param input_segment: torch.Size([batch_size, article_len])\n",
    "        :return:\n",
    "            logprobs | torch.Size([batch_size, summary_len, vocab_size])\n",
    "        \"\"\"\n",
    "\n",
    "        cls_mask = (input_ids == self.artBOS.idx)\n",
    "\n",
    "        # position ids | torch.Size([batch_size, article_len])\n",
    "        pos_ids = torch.arange(\n",
    "            0,\n",
    "            self.article_len,\n",
    "            dtype=torch.long,\n",
    "            device=input_ids.device\n",
    "        ).unsqueeze(0).repeat(len(input_ids), 1)\n",
    "        # extract bert embeddings | torch.Size([batch_size, article_len, d_bert])\n",
    "        enc_output = self.encoder(input_ids, input_mask, input_segment, pos_ids)\n",
    "\n",
    "        if self.features:\n",
    "            temp_features = self.features(input_features)\n",
    "            draft_logprobs = self.decoder(torch.cat([enc_output[cls_mask], temp_features], dim=-1))\n",
    "        else:\n",
    "            draft_logprobs = self.decoder(enc_output[cls_mask])\n",
    "\n",
    "        return draft_logprobs\n",
    "\n",
    "    def evaluate(self, input_ids, input_mask, input_segment, input_features=None):\n",
    "        \"\"\" Eval for 1st stage of model\n",
    "        :param input_ids: torch.Size([batch_size, article_len])\n",
    "        :param input_mask: torch.Size([batch_size, article_len])\n",
    "        :param input_segment: torch.Size([batch_size, article_len])\n",
    "        :return:\n",
    "            draft_ids | torch.Size([batch_size, summary_len])\n",
    "        \"\"\"\n",
    "\n",
    "        cls_mask = (input_ids == self.artBOS.idx)\n",
    "\n",
    "        # position ids | torch.Size([batch_size, article_len])\n",
    "        pos_ids = torch.arange(\n",
    "            0,\n",
    "            self.article_len,\n",
    "            dtype=torch.long,\n",
    "            device=input_ids.device\n",
    "        ).unsqueeze(0).repeat(len(input_ids), 1)\n",
    "        # extract bert embeddings | torch.Size([batch_size, article_len, d_bert])\n",
    "        enc_output = self.encoder(input_ids, input_mask, input_segment, pos_ids)\n",
    "\n",
    "        ans = []\n",
    "        for eo, cm in zip(enc_output, cls_mask):\n",
    "            if self.features:\n",
    "                scores = self.decoder.evaluate(torch.cat([eo[cm], self.features(input_features)], dim=-1))\n",
    "            else:\n",
    "                scores = self.decoder.evaluate(eo[cm])\n",
    "            ans.append(scores)\n",
    "        return ans\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x).squeeze(-1)\n",
    "        scores = self.sigmoid(x)\n",
    "        return scores\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        x = self.linear1(x).squeeze(-1)\n",
    "        scores = self.sigmoid(x)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def load_model(model_type, froze_strategy, article_len, additional_features=False):\n",
    "    model = Summarizer(model_type, article_len, additional_features)\n",
    "    model.expand_posembs_ifneed()\n",
    "    # Load intermediate model\n",
    "    #     model.load('temp')\n",
    "    model.froze_backbone(froze_strategy)\n",
    "    model.unfroze_head()\n",
    "    print(f'Parameters {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \"\"\" Custom Train Dataset for data with additional features\n",
    "        First preprocess all the data and then give out the batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len, additional_features=False):\n",
    "        self.df = dataframe\n",
    "        self.data = []\n",
    "        self.names = list(set(dataframe['pmid'].values))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "\n",
    "        for name in tqdm(self.names):\n",
    "            ex = self.df[self.df['pmid'] == name]\n",
    "            papers = ex['sentence'].values\n",
    "            features = np.nan_to_num(\n",
    "                ex[['sent_id', 'sent_type', 'r_abs', 'num_refs',\n",
    "                    'mean_r_fig', 'mean_r_tab',\n",
    "                    'min_r_fig', 'min_r_tab',\n",
    "                    'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "            ) if additional_features else None\n",
    "            total_sents = 0\n",
    "            while total_sents < len(papers):\n",
    "                magic = max(0, total_sents - 5)\n",
    "                article_ids, article_mask, article_segment, n_sents = preprocess_paper_bert(\n",
    "                    papers[magic:], self.article_len, self.tokenizer\n",
    "                )\n",
    "                if n_sents <= 5:\n",
    "                    total_sents += 1\n",
    "                    continue\n",
    "                target_scores = ex['score'].values[magic:magic + n_sents] / 100\n",
    "                self.data.append(\n",
    "                    (article_ids, article_mask, article_segment, target_scores, features[magic:magic + n_sents])\n",
    "                    if additional_features else\n",
    "                    (article_ids, article_mask, article_segment, target_scores)\n",
    "                )\n",
    "                total_sents = magic + n_sents\n",
    "\n",
    "        self.n_examples = len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    \"\"\" Custom Valid/Test Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len, additional_features=False):\n",
    "        self.df = dataframe\n",
    "        self.n_examples = len(set(dataframe['pmid'].values))\n",
    "        print(self.n_examples)\n",
    "        self.names = list(set(dataframe['pmid'].values))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "        self.additional_features = additional_features\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.names[idx]\n",
    "        ex = self.df[self.df['pmid'] == idx]\n",
    "        paper = ex['sentence'].values\n",
    "        features = np.nan_to_num(\n",
    "            ex[['sent_id', 'sent_type', 'r_abs',\n",
    "                'num_refs', 'mean_r_fig', 'mean_r_tab',\n",
    "                'min_r_fig', 'min_r_tab',\n",
    "                'max_r_fig', 'max_r_tab']].values.astype(float)\n",
    "        ) if self.additional_features else None\n",
    "\n",
    "        article_ids, article_mask, article_segment, n_sents = preprocess_paper_bert(\n",
    "            paper, self.article_len, self.tokenizer\n",
    "        )\n",
    "\n",
    "        # form target\n",
    "        target_scores = ex['score'].values[:n_sents] / 100\n",
    "        if self.additional_features:\n",
    "            return article_ids, article_mask, article_segment, target_scores, features[:n_sents]\n",
    "        else:\n",
    "            return article_ids, article_mask, article_segment, target_scores\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler, ExponentialLR\n",
    "\n",
    "\n",
    "class CustomScheduler(_LRScheduler):\n",
    "    timestep: int = 0\n",
    "\n",
    "    def __init__(self, optimizer, gamma, warmup=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.after_warmup = ExponentialLR(optimizer, gamma=gamma)\n",
    "        self.initial_lrs = [p_group['lr'] for p_group in self.optimizer.param_groups]\n",
    "        self.warmup = 0 if warmup is None else warmup\n",
    "        super(CustomScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.timestep * group_init_lr / self.warmup for group_init_lr in\n",
    "                self.initial_lrs] if self.timestep < self.warmup else self.after_warmup.get_lr()\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if self.timestep < self.warmup:\n",
    "            self.timestep += 1\n",
    "            super(CustomScheduler, self).step(epoch)\n",
    "        else:\n",
    "            self.after_warmup.step(epoch)\n",
    "\n",
    "\n",
    "class NoamScheduler(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup):\n",
    "        assert warmup > 0\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lrs = [p_group['lr'] for p_group in self.optimizer.param_groups]\n",
    "        self.warmup = warmup\n",
    "        self.timestep = 0\n",
    "        super(NoamScheduler, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        noam_lr = self.get_noam_lr()\n",
    "        return [group_init_lr * noam_lr for group_init_lr in self.initial_lrs]\n",
    "\n",
    "    def get_noam_lr(self):\n",
    "        return min(self.timestep ** -0.5, self.timestep * self.warmup ** -1.5)\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        self.timestep += 1\n",
    "        super(NoamScheduler, self).step(epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_collate_fn(batch_data):\n",
    "    \"\"\" Function to pull batch for train\n",
    "    :param batch_data: list of `TrainDataset` Examples\n",
    "    :return:\n",
    "        one batch of data\n",
    "    \"\"\"\n",
    "    # print(str(batch_data))\n",
    "    data0, data1, data2, data3 = list(zip(*batch_data))\n",
    "    return (\n",
    "        torch.tensor(data0, dtype=torch.long),\n",
    "        torch.tensor(data1, dtype=torch.long),\n",
    "        torch.tensor(data2, dtype=torch.long),\n",
    "        [torch.tensor(e, dtype=torch.float) for e in data3]\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_collate_fn(batch_data):\n",
    "    \"\"\" Function to pull batch for valid/test\n",
    "    :param batch_data: list of `EvalDataset` Examples\n",
    "    :return:\n",
    "        one batch of data\n",
    "    \"\"\"\n",
    "    # print(str(batch_data))\n",
    "    data0, data1, data2, data3 = list(zip(*batch_data))\n",
    "    return (\n",
    "        torch.tensor(data0, dtype=torch.long),\n",
    "        torch.tensor(data1, dtype=torch.long),\n",
    "        torch.tensor(data2, dtype=torch.long),\n",
    "        [torch.tensor(e, dtype=torch.float) for e in data3]\n",
    "    )\n",
    "\n",
    "\n",
    "def train_collate_fn_ft(batch_data):\n",
    "    # print(str(batch_data))\n",
    "    data0, data1, data2, data3, data4 = list(zip(*batch_data))\n",
    "    return (\n",
    "        torch.tensor(data0, dtype=torch.long),\n",
    "        torch.tensor(data1, dtype=torch.long),\n",
    "        torch.tensor(data2, dtype=torch.long),\n",
    "        [torch.tensor(e, dtype=torch.float) for e in data3],\n",
    "        [torch.tensor(e, dtype=torch.float) for e in data4]\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_collate_fn_ft(batch_data):\n",
    "    # print(str(batch_data))\n",
    "    data0, data1, data2, data3, data4 = list(zip(*batch_data))\n",
    "\n",
    "    return (\n",
    "        torch.tensor(data0, dtype=torch.long),\n",
    "        torch.tensor(data1, dtype=torch.long),\n",
    "        torch.tensor(data2, dtype=torch.long),\n",
    "        [torch.tensor(e, dtype=torch.float) for e in data3],\n",
    "        [torch.tensor(e, dtype=torch.float) for e in data4]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_loader(dataset, batch_size, collate_fn):\n",
    "    return DataLoader(\n",
    "        dataset=dataset, batch_size=batch_size, shuffle=False,\n",
    "        pin_memory=True, collate_fn=collate_fn, num_workers=cfg.num_workers\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataloaders(train, val, batch_size, article_len, tokenizer, additional_features=False):\n",
    "    logging.info('Creating train dataset...')\n",
    "    train_ds = TrainDataset(train, tokenizer, article_len, additional_features)\n",
    "\n",
    "    logging.info('Applying loader functions to train...')\n",
    "    train_dl = create_loader(\n",
    "        train_ds, batch_size, train_collate_fn_ft if additional_features else train_collate_fn\n",
    "    )\n",
    "\n",
    "    logging.info('Creating val dataset...')\n",
    "    val_ds = EvalDataset(val, tokenizer, article_len, additional_features)\n",
    "\n",
    "    logging.info('Applying loader functions to val...')\n",
    "    val_dl = create_loader(\n",
    "        val_ds, batch_size, eval_collate_fn_ft if additional_features else eval_collate_fn\n",
    "    )\n",
    "\n",
    "    return train_dl, val_dl\n",
    "\n",
    "\n",
    "def get_tools(model, enc_lr, dec_lr, warmup,\n",
    "              weight_decay, clip_value,\n",
    "              accumulation_interval):\n",
    "    # TODO fix for Roberta model\n",
    "    enc_parameters = [\n",
    "        param for name, param in model.named_parameters()\n",
    "        if param.requires_grad and name.startswith('bert.')\n",
    "    ]\n",
    "    dec_parameters = [\n",
    "        param for name, param in model.named_parameters()\n",
    "        if param.requires_grad and not name.startswith('bert.')\n",
    "    ]\n",
    "    optimizer = AdamW([\n",
    "        dict(params=enc_parameters, lr=enc_lr),\n",
    "        dict(params=dec_parameters, lr=dec_lr),\n",
    "    ], weight_decay=weight_decay)\n",
    "    optimizer.clip_value = clip_value\n",
    "    optimizer.accumulation_interval = accumulation_interval\n",
    "\n",
    "    scheduler = NoamScheduler(optimizer, warmup=warmup)\n",
    "    criter = MSELoss()\n",
    "\n",
    "    return optimizer, scheduler, criter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criter, device, writer):\n",
    "    model.eval()\n",
    "    loss_val = 0\n",
    "    mean_sents = 0\n",
    "    szs = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), leave=False)\n",
    "    for batch in pbar:\n",
    "        input_ids, input_mask, input_segment, target_scores = [\n",
    "            (x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch\n",
    "        ]\n",
    "        sizes = [dc.shape[0] for dc in target_scores]\n",
    "        mean_sents += sum(sizes)\n",
    "        szs += len(sizes)\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "\n",
    "        # forward pass\n",
    "        draft_probs = model(\n",
    "            input_ids, input_mask, input_segment,\n",
    "        )\n",
    "        #print(draft_probs.shape, target_scores.shape)\n",
    "\n",
    "        # loss\n",
    "        loss = criter(draft_probs, target_scores, )\n",
    "\n",
    "        # record a loss value\n",
    "        pbar.set_description(f\"loss:{loss.item():.5f}\")\n",
    "        loss_val += loss.item()\n",
    "        writer.add_scalar(f\"Eval/loss\", loss.item(), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "    print(\"Val loss:\", loss_val / len(dataloader))\n",
    "    print(\"Mean sent len:\", mean_sents / szs)\n",
    "    # save model, just in case\n",
    "    model.save('validated_weights.pth')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_ft(model, dataloader, criter, device, writer):\n",
    "    model.eval()\n",
    "    loss_val = 0\n",
    "    mean_sents = 0\n",
    "    szs = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), leave=False)\n",
    "    for batch in pbar:\n",
    "        input_ids, input_mask, input_segment, target_scores, input_features = [\n",
    "            (x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch\n",
    "        ]\n",
    "        sizes = [dc.shape[0] for dc in target_scores]\n",
    "        mean_sents += sum(sizes)\n",
    "        szs += len(sizes)\n",
    "        input_features = torch.cat(input_features).to(device)\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "\n",
    "        # forward pass\n",
    "        draft_probs = model(\n",
    "            input_ids, input_mask, input_segment, input_features,\n",
    "        )\n",
    "        #print(draft_probs.shape, target_scores.shape)\n",
    "\n",
    "        # loss\n",
    "        loss = criter(draft_probs, target_scores, )\n",
    "\n",
    "        # record a loss value\n",
    "        pbar.set_description(f\"loss:{loss.item():.5f}\")\n",
    "        loss_val += loss.item()\n",
    "        writer.add_scalar(f\"Eval/loss\", loss.item(), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "    print(\"Val loss:\", loss_val / len(dataloader))\n",
    "    print(\"Mean sent len:\", mean_sents / szs)\n",
    "    # save model, just in case\n",
    "    model.save('validated_weights.pth')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading or training the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ARTICLE_LENGTH = 512\n",
    "\n",
    "model = load_model(\"bert\", \"froze_all\", ARTICLE_LENGTH, additional_features=True)\n",
    "model, device = setup_cuda_device(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create dataloaders and start training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "def load_or_train_model(model, device, additional_features=False):\n",
    "    model_name = f'learn_simple_berta_{additional_features}.pth'.lower()\n",
    "    MODEL_PATH = f'{os.path.expanduser(cfg.weights_path)}/{model_name}'\n",
    "    ! rm {MODEL_PATH}\n",
    "\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        logging.info(f'Loading model {MODEL_PATH}')\n",
    "        model.load(\"learn_simple_berta\")\n",
    "        model, device = setup_cuda_device(model)\n",
    "    else:\n",
    "        logging.info('Create dataloaders...')\n",
    "        train_loader, valid_loader = get_dataloaders(\n",
    "            train, val, 4, ARTICLE_LENGTH, model.tokenizer, additional_features\n",
    "        )\n",
    "\n",
    "        writer = SummaryWriter(log_dir=os.path.expanduser(cfg.log_path))\n",
    "        writer.train_step, writer.eval_step = 0, 0\n",
    "\n",
    "        optimizer, scheduler, criter = get_tools(model, 0.00001, 0.001, 5, 0.005, 1.0, 1)\n",
    "\n",
    "        for epoch in range(1, 20 + 1):\n",
    "            logging.info(f\"{epoch} epoch training...\")\n",
    "            if additional_features:\n",
    "                model, optimizer, scheduler, writer = train_fun_ft(\n",
    "                    model, train_loader, optimizer, scheduler,\n",
    "                    criter, device, writer\n",
    "                )\n",
    "            else:\n",
    "                model, optimizer, scheduler, writer = train_fun(\n",
    "                    model, train_loader, optimizer, scheduler,\n",
    "                    criter, device, writer\n",
    "                )\n",
    "\n",
    "            logging.info(f\"{epoch} epoch validation...\")\n",
    "            if additional_features:\n",
    "                model = evaluate_ft(model, valid_loader, criter, device, writer, )\n",
    "            else:\n",
    "                model = evaluate(model, valid_loader, criter, device, writer, )\n",
    "\n",
    "        logging.info(f'Save trained model to {model_name}')\n",
    "        model.save(model_name)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_or_train_model(model, device, additional_features=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate model performance\n",
    "TODO: update model with additional features analysis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pick the first value\n",
    "ex = val[val['pmid'] == list(val['pmid'])[0]]\n",
    "paper = ex['sentence'].values\n",
    "article_ids, article_mask, article_segment, n_sents = preprocess_paper_bert(\n",
    "    paper, ARTICLE_LENGTH, model.tokenizer\n",
    ")\n",
    "res_sents = paper[:n_sents]\n",
    "scores = ex['score'].values[:n_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([article_ids]).to(device)\n",
    "input_mask = torch.tensor([article_mask]).to(device)\n",
    "input_segment = torch.tensor([article_segment]).to(device)\n",
    "draft_probs = model(input_ids, input_mask, input_segment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_show_df = pd.DataFrame(\n",
    "    dict(sentence=res_sents, ideal_score=scores / 100, res_score=draft_probs.cpu().detach().numpy())\n",
    ")\n",
    "display(to_show_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "' '.join(to_show_df[to_show_df['res_score'] > 0.07]['sentence'].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_show_df.to_csv(f'{cfg.base_path}/show_scores.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inspect scores\n",
    "First, let's see if the model trained well.\\\n",
    "Then will count an `MSE` score on some real example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 'model' not in globals():\n",
    "    model = load_model(\"bert\", \"froze_all\", ARTICLE_LENGTH, additional_features=True)\n",
    "    model.load(\"learn_simple_berta\")\n",
    "    model, device = setup_cuda_device(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "REF_SCORES_PATH = os.path.expanduser(f\"{cfg.base_path}/refs_and_scores.csv\")\n",
    "\n",
    "! rm {REF_SCORES_PATH}\n",
    "\n",
    "if os.path.exists(REF_SCORES_PATH):\n",
    "    final_ref_show_df = pd.read_csv(REF_SCORES_PATH)\n",
    "else:\n",
    "    if 'train_loader' not in globals() or 'valid_loader' not in globals():\n",
    "        train_loader, valid_loader = get_dataloaders(train, val, 4, ARTICLE_LENGTH, model.tokenizer)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=os.path.expanduser(cfg.log_path))\n",
    "    writer.train_step, writer.eval_step = 0, 0\n",
    "    optimizer, scheduler, criter = get_tools(model, 0.00001, 0.001, 5, 0.005, 1.0, 1)\n",
    "    model = evaluate(model, train_loader, criter, device, writer)\n",
    "\n",
    "    # model = evaluate_ft(model, valid_loader, criter, device, 0, writer, False,)    \n",
    "    to_show_ref = pd.merge(train_df, ref_sents_df[['ref_pmid', 'sentence']],\n",
    "                           left_on=['pmid'], right_on=['ref_pmid'])\n",
    "    to_show_ref = to_show_ref.rename(columns=dict(sentence_x='sentence', sentence_y='ref_sentence'))\n",
    "    to_show_ref = to_show_ref[['pmid', 'sentence', 'ref_sentence', 'score']]\n",
    "    final_ref_show_dic = dict(pmid=[], sentence=[], ref_sentence=[], score=[])\n",
    "    ite = [(pmid, sent) for pmid, sent in to_show_ref[['pmid', 'sentence']].values]\n",
    "\n",
    "    for pmid, sent in tqdm(set(ite)):\n",
    "        refs_df = to_show_ref[(to_show_ref['pmid'] == pmid) & (to_show_ref['sentence'] == sent)]\n",
    "        final_ref_show_dic['pmid'].append(pmid)\n",
    "        final_ref_show_dic['sentence'].append(sent)\n",
    "        final_ref_show_dic['ref_sentence'].append(\" \".join(refs_df['ref_sentence'].values))\n",
    "        final_ref_show_dic['score'].append(refs_df['score'].values[0])\n",
    "    final_ref_show_df = pd.DataFrame(final_ref_show_dic)\n",
    "    final_ref_show_df.to_csv(REF_SCORES_PATH, index=False)\n",
    "\n",
    "display(final_ref_show_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_test = final_ref_show_df[final_ref_show_df['pmid'].isin(set(val['pmid'].values))]\n",
    "display(to_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = dict(pmid=[], sentence=[], ref_sentences=[], score=[], res_score=[])\n",
    "\n",
    "for id in tqdm(set(to_test['pmid'].values)):\n",
    "    ex = to_test[to_test['pmid'] == id]\n",
    "    paper = ex['sentence'].values\n",
    "    article_ids, article_mask, article_segment, n_sents = preprocess_paper_bert(\n",
    "        paper, ARTICLE_LENGTH, model.tokenizer\n",
    "    )\n",
    "    res_sents = paper[:n_sents]\n",
    "    scores = ex['score'].values[:n_sents] / 100\n",
    "    input_ids = torch.tensor([article_ids]).to(device)\n",
    "    input_mask = torch.tensor([article_mask]).to(device)\n",
    "    input_segment = torch.tensor([article_segment]).to(device)\n",
    "    draft_probs = model(input_ids, input_mask, input_segment, )\n",
    "    for sent, sc, res_sc in zip(res_sents, scores, draft_probs.cpu().detach().numpy()):\n",
    "        res['pmid'].append(id)\n",
    "        res['sentence'].append(sent)\n",
    "        res['ref_sentences'].append(ex['ref_sentence'].values[0])\n",
    "        res['score'].append(sc)\n",
    "        res['res_score'].append(res_sc)\n",
    "res_df = pd.DataFrame(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_df.to_csv(f\"{cfg.base_path}/saved_example_refs.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diff = ((res_df['score'].values - res_df['res_score'].values) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diff ** 0.5  #The MSE score!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Investigate MSE\n",
    "Let's see what is the original score distribution to understand the quality of `MSE`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "res = Counter(list(train_df['score'].values))\n",
    "\n",
    "plt.hist(res.values(), bins=range(2, 20))\n",
    "plt.title('Scores')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ref_sents_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paper_ref = sentences_df[sentences_df['pmid'] == 14706945]['sentence'].values\n",
    "paper_ref"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "papers_to_check = list(set(ref_sents_df[ref_sents_df['pmid'] == 8300981]['ref_pmid'].values))\n",
    "papers_to_check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A test of several paper summarization into review one"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Summarizer('bert', ARTICLE_LENGTH)\n",
    "# model.load('bert_sum')\n",
    "model.load('learn_simple_berta')\n",
    "model.froze_backbone(\"froze_all\")\n",
    "model.unfroze_head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model, device = setup_cuda_device(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Features-related evaluation is commented right now.\n",
    "test_stat = dict(rev_pmid=[], sent_num=[], true_rouge=[], diff_papers=[])  #'rouge': [],\n",
    "inter = set(sentences_df['pmid'].values) & set(ref_sents_df['ref_pmid'].values)\n",
    "review_papers = list(set(ref_sents_df[ref_sents_df['ref_pmid'].isin(inter)]['pmid'].values))\n",
    "print('Review papers', len(review_papers))\n",
    "cnt = 0\n",
    "\n",
    "for rev_id in tqdm(review_papers):\n",
    "    #     print(f\"\\r{rev_id} {cnt} / {len(review_papers)}\", end=\"\")\n",
    "    cnt += 1\n",
    "    paper_ref = sentences_df[sentences_df['pmid'] == rev_id]['sentence'].values\n",
    "    papers_to_check = list(set(ref_sents_df[ref_sents_df['pmid'] == rev_id]['ref_pmid'].values))\n",
    "    result = {'pmid': [], 'sentence': [], 'score': []}\n",
    "    for paper_id in papers_to_check:\n",
    "        ex = test[test['pmid'] == paper_id]\n",
    "        paper = ex['sentence'].values\n",
    "        #features = np.nan_to_num(ex[['sent_id', 'sent_type', 'r_abs',\n",
    "        #                   'num_refs', 'mean_r_fig', 'mean_r_tab',\n",
    "        #                   'min_r_fig', 'min_r_tab',\n",
    "        #                   'max_r_fig', 'max_r_tab']].values.astype(float))\n",
    "        total_sents = 0\n",
    "        while total_sents < len(paper):\n",
    "            magic = max(0, total_sents - 5)\n",
    "            article_ids, article_mask, article_segment, n_sents = preprocess_paper_bert(\n",
    "                paper[magic:], ARTICLE_LENGTH, model.tokenizer\n",
    "            )\n",
    "            if n_sents <= 5:\n",
    "                total_sents += 1\n",
    "                continue\n",
    "            old_total = total_sents\n",
    "            total_sents = magic + n_sents\n",
    "            input_ids = torch.tensor([article_ids]).to(device)\n",
    "            input_mask = torch.tensor([article_mask]).to(device)\n",
    "            input_segment = torch.tensor([article_segment]).to(device)\n",
    "            #input_features = [torch.tensor(e, dtype=torch.float) for e in features[magic:total_sents]]\n",
    "            #input_features = torch.stack(input_features).to(device)\n",
    "            #print(input_features)\n",
    "            draft_probs = model(\n",
    "                input_ids, input_mask, input_segment,  #input_features,\n",
    "            )\n",
    "            result['pmid'].extend([paper_id] * (total_sents - old_total))\n",
    "            result['sentence'].extend(list(paper[old_total:total_sents]))\n",
    "            result['score'].extend(list(draft_probs.cpu().detach().numpy())[old_total - magic:])\n",
    "    res_df = pd.DataFrame(result)\n",
    "    sorted_arr = sorted(list(res_df['score'].values))\n",
    "    for i in range(5, 103, 5):\n",
    "        if len(sorted_arr) < i:\n",
    "            break\n",
    "        threshold = sorted_arr[-i]\n",
    "        final_text = res_df[res_df['score'] >= threshold][['pmid', 'sentence']]\n",
    "        #mean_score = 0\n",
    "        #num = 0\n",
    "        #for sent in final_text['sentence'].values:\n",
    "        #    for ref_sent in paper_ref:\n",
    "        #        try:\n",
    "        #            mean_score += get_rouge(sent, ref_sent)\n",
    "        #            num += 1\n",
    "        #        except Exception:\n",
    "        #            continue\n",
    "        #mean_score /= num\n",
    "        real_score = get_rouge(\" \".join(final_text['sentence'].values), \" \".join(paper_ref))\n",
    "        test_stat['rev_pmid'].append(rev_id)\n",
    "        test_stat['sent_num'].append(i)\n",
    "        #print(len(\" \".join(final_text['sentence'].values)), len(\" \".join(paper_ref)))\n",
    "\n",
    "        #test_stat['rouge'].append(mean_score)\n",
    "        test_stat['true_rouge'].append(real_score)\n",
    "        test_stat['diff_papers'].append(len(set(final_text['pmid'])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(*[len(arr) for key, arr in test_stat.items()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_stat_df = pd.DataFrame(test_stat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_stat_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_stat_df = test_stat_df[test_stat_df['rev_pmid'] != 29574033]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_stat_df.to_csv(f\"{cfg.base_path}/simple_right_test_on_review.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(result['sentence'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(result['score'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = sorted(list(res_df['score'].values))[-5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_text = res_df[res_df['score'] >= threshold][['pmid', 'sentence']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(set(final_text['pmid']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\" \".join(final_text['sentence'].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_score = 0\n",
    "num = 0\n",
    "for sent in final_text['sentence'].values:\n",
    "    for ref_sent in paper_ref:\n",
    "        mean_score += get_rouge(sent, ref_sent)\n",
    "        num += 1\n",
    "mean_score /= num"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\" \".join(final_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inter = list(set(sentences_df['pmid'].values) & set(ref_sents_df['ref_pmid'].values))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "26194312 in inter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ref_sents_df[ref_sents_df['ref_pmid'] == 25559091]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quality plots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_df = pd.concat([df_1, df_2])\n",
    "draw_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rouge_means = []\n",
    "rouge_err = []\n",
    "papers_means = []\n",
    "papers_err = []\n",
    "\n",
    "for i in range(5, 103, 5):\n",
    "    tmp = test_stat_df.groupby(['sent_num']).get_group(i)\n",
    "    rouge_means.append(tmp['rouge'].mean())\n",
    "    rouge_err.append(tmp['rouge'].std())\n",
    "    papers_means.append(tmp['diff_papers'].mean())\n",
    "    papers_err.append(tmp['diff_papers'].std())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.errorbar(list(range(5, 103, 5)), rouge_means, yerr=rouge_err, fmt='-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.errorbar(list(range(5, 103, 5)), papers_means, yerr=papers_err, fmt='-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"sent_num\", y=\"rouge\", kind=\"box\", hue='model', aspect=1.7, color='lightblue',\n",
    "            data=draw_df).set_axis_labels(\"ЧИСЛО ПРЕДЛОЖЕНИЙ\", \"ROUGE, %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"sent_num\", y=\"diff_papers\", kind=\"box\", aspect=1.5, color='lightblue',\n",
    "            data=test_stat_df).set_axis_labels(\"ЧИСЛО ПРЕДЛОЖЕНИЙ\", \"ЧИСЛО СТАТЕЙ В РЕЗЮМЕ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"sent_num\", y=\"true_rouge\", kind=\"box\", aspect=1.7, hue='model', color='lightblue',\n",
    "            data=draw_df).set_axis_labels(\"ЧИСЛО ПРЕДЛОЖЕНИЙ\", \"ROUGE, %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}